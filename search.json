[{"path":"/articles/kmedoid.html","id":"intro","dir":"Articles","previous_headings":"","what":"1. Introduction","title":"kmed: Distance-Based K-Medoids","text":"kmed package designed analyse k-medoids based clustering. features include: Manhattan weighted range squared Euclidean weighted range squared Euclidean weighted squared range squared Euclidean weighted variance unweighted squared Euclidean simple matching co-occurrence Gower Wishart Podani Huang Harikumar PV Ahmad Dey Simple fast k-medoids K-medoids Rank k-medoids Increasing number clusters k-medoids simple k-medoids Silhouette Centroid-based shadow value Medoid-based shadow value relative criteria (bootstrap) pca biplot marked barplot","code":""},{"path":[]},{"path":"/articles/kmedoid.html","id":"a--numerical-variables-distnumeric","dir":"Articles","previous_headings":"2. Distance Computation","what":"2.A. Numerical variables (distNumeric)","title":"kmed: Distance-Based K-Medoids","text":"distNumeric function can applied calculate numerical distances. four distance options, namely Manhattan weighted range (mrw), squared Euclidean weighted range (ser), squared Euclidean weighted squared range (ser.2), squared Euclidean weighted variance (sev), unweighted squared Euclidean (se). distNumeric function provides method desired distance method can selected. default method mrw. distance computation numerical variable data set performed iris data set. example manual calculation numerical distances applied first second objects introduce distNumeric function .","code":"library(kmed) iris[1:3,] ##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1          5.1         3.5          1.4         0.2  setosa ## 2          4.9         3.0          1.4         0.2  setosa ## 3          4.7         3.2          1.3         0.2  setosa"},{"path":"/articles/kmedoid.html","id":"mrw","dir":"Articles","previous_headings":"2. Distance Computation > 2.A. Numerical variables (distNumeric)","what":"2.A.1. Manhattan weighted by range (method = \"mrw\")","title":"kmed: Distance-Based K-Medoids","text":"applying distNumeric function method = \"mrw\", distance among objects iris data set can obtained. Manhattan weighted range distance objects 1 2 0.2638889. calculate distance, range variable computed. , distance objects 1 2 based data (Back Intoduction)","code":"num <- as.matrix(iris[,1:4]) rownames(num) <- rownames(iris) #calculate the Manhattan weighted by range distance of all iris objects mrwdist <- distNumeric(num, num) #show the distance among objects 1 to 3 mrwdist[1:3,1:3] ##           1         2         3 ## 1 0.0000000 0.2638889 0.2530603 ## 2 0.2638889 0.0000000 0.1558380 ## 3 0.2530603 0.1558380 0.0000000 #extract the range of each variable apply(num, 2, function(x) max(x)-min(x)) ## Sepal.Length  Sepal.Width Petal.Length  Petal.Width  ##          3.6          2.4          5.9          2.4 #the distance between objects 1 and 2 abs(5.1-4.9)/3.6 + abs(3.5 - 3.0)/2.4 + abs(1.4-1.4)/5.9 + abs(0.2-0.2)/2.4 ## [1] 0.2638889 ##   Sepal.Length Sepal.Width Petal.Length Petal.Width ## 1          5.1         3.5          1.4         0.2 ## 2          4.9         3.0          1.4         0.2"},{"path":"/articles/kmedoid.html","id":"ser","dir":"Articles","previous_headings":"2. Distance Computation > 2.A. Numerical variables (distNumeric)","what":"2.A.2. squared Euclidean weighted by range (method = \"ser\")","title":"kmed: Distance-Based K-Medoids","text":"squared Euclidean weighted range distance objects 1 2 0.11527778. obtained (Back Intoduction)","code":"#calculate the squared Euclidean weighthed by range distance of all iris objects serdist <- distNumeric(num, num, method = \"ser\") #show the distance among objects 1 to 3 serdist[1:3,1:3] ##            1          2          3 ## 1 0.00000000 0.11527778 0.08363936 ## 2 0.11527778 0.00000000 0.02947269 ## 3 0.08363936 0.02947269 0.00000000 #the distance between objects 1 and 2 (5.1-4.9)^2/3.6 + (3.5 - 3.0)^2/2.4 + (1.4-1.4)^2/5.9 + (0.2-0.2)^2/2.4 ## [1] 0.1152778"},{"path":"/articles/kmedoid.html","id":"ser-2","dir":"Articles","previous_headings":"2. Distance Computation > 2.A. Numerical variables (distNumeric)","what":"2.A.3. squared Euclidean weighted by squared range (method = \"ser.2\")","title":"kmed: Distance-Based K-Medoids","text":"squared Euclidean weighted squared range distance objects 1 2 0.04648920 computed data (Back Intoduction)","code":"#calculate the squared Euclidean weighthed by squared range distance of  #all iris objects ser.2dist <- distNumeric(num, num, method = \"ser.2\") #show the distance among objects 1 to 3 ser.2dist[1:3,1:3] ##            1          2          3 ## 1 0.00000000 0.04648920 0.02825795 ## 2 0.04648920 0.00000000 0.01031814 ## 3 0.02825795 0.01031814 0.00000000 (5.1-4.9)^2/3.6^2 + (3.5 - 3.0)^2/2.4^2 + (1.4-1.4)^2/5.9^2 + (0.2-0.2)^2/2.4^2 ## [1] 0.0464892 ##   Sepal.Length Sepal.Width Petal.Length Petal.Width ## 1          5.1         3.5          1.4         0.2 ## 2          4.9         3.0          1.4         0.2"},{"path":"/articles/kmedoid.html","id":"a-4--squared-euclidean-weighted-by-variance-method-sev","dir":"Articles","previous_headings":"2. Distance Computation > 2.A. Numerical variables (distNumeric)","what":"2.A.4. squared Euclidean weighted by variance (method = \"sev\")","title":"kmed: Distance-Based K-Medoids","text":"squared Euclidean weighted variance distance objects 1 2 1.3742671. compute distance, variance variable calculated. , distance objects 1 2 (Back Intoduction)","code":"#calculate the squared Euclidean weighthed by variance distance of  #all iris objects sevdist <- distNumeric(num, num, method = \"sev\") #show the distance among objects 1 to 3 sevdist[1:3,1:3] ##           1         2         3 ## 1 0.0000000 1.3742671 0.7102849 ## 2 1.3742671 0.0000000 0.2720932 ## 3 0.7102849 0.2720932 0.0000000 #calculate the range of each variable apply(num[,1:4], 2, function(x) var(x)) ## Sepal.Length  Sepal.Width Petal.Length  Petal.Width  ##    0.6856935    0.1899794    3.1162779    0.5810063 (5.1-4.9)^2/0.6856935 + (3.5 - 3.0)^2/0.1899794 + (1.4-1.4)^2/3.1162779 +   (0.2-0.2)^2/0.5810063 ## [1] 1.374267"},{"path":"/articles/kmedoid.html","id":"se","dir":"Articles","previous_headings":"2. Distance Computation > 2.A. Numerical variables (distNumeric)","what":"2.A.5. squared Euclidean (method = \"se\")","title":"kmed: Distance-Based K-Medoids","text":"squared Euclidean distance objects 1 2 0.29. computed (Back Intoduction)","code":"#calculate the squared Euclidean distance of all iris objects sedist <- distNumeric(num, num, method = \"se\") #show the distance among objects 1 to 3 sedist[1:3,1:3] ##      1    2    3 ## 1 0.00 0.29 0.26 ## 2 0.29 0.00 0.09 ## 3 0.26 0.09 0.00 (5.1-4.9)^2 + (3.5 - 3.0)^2 + (1.4-1.4)^2 + (0.2-0.2)^2 ## [1] 0.29"},{"path":"/articles/kmedoid.html","id":"b--binary-or-categorical-variables","dir":"Articles","previous_headings":"2. Distance Computation","what":"2.B. Binary or Categorical variables","title":"kmed: Distance-Based K-Medoids","text":"two functions calculate binary categorical variables. first matching compute simple matching distance second cooccur calculate co-occurrence distance. introduce functions , bin data set generated.","code":"set.seed(1) bin <- matrix(sample(1:2, 4*2, replace = TRUE), 4, 2) rownames(bin) <- 1:nrow(bin) colnames(bin) <- c(\"x\", \"y\")"},{"path":"/articles/kmedoid.html","id":"sm","dir":"Articles","previous_headings":"2. Distance Computation > 2.B. Binary or Categorical variables","what":"2.B.1. Simple matching (matching)","title":"kmed: Distance-Based K-Medoids","text":"matching function calculates simple matching distance two data sets. two data sets identical, functions calculates distance among objects within data set. simple matching distance equal proportion mis-match categories. example simple matching distance, distance objects 1 2 calculated distance objects 1 2, 0.5, produced one mis-match one match categories two variables (x y) bin data set. x1 equal x2, instance, score 0. Meanwile, x1 equal x2, score 1. scores also valid y variable. Hence, distance objects 1 2 (0+1)/2 equal 1/2. (Back Intoduction)","code":"bin ##   x y ## 1 1 2 ## 2 2 1 ## 3 1 1 ## 4 1 1 #calculate simple matching distance matching(bin, bin) ##     1   2   3   4 ## 1 0.0 1.0 0.5 0.5 ## 2 1.0 0.0 0.5 0.5 ## 3 0.5 0.5 0.0 0.0 ## 4 0.5 0.5 0.0 0.0 ((1 == 1) + (1 == 2))/ 2 ## [1] 0.5"},{"path":"/articles/kmedoid.html","id":"cooc","dir":"Articles","previous_headings":"2. Distance Computation > 2.B. Binary or Categorical variables","what":"2.B.2. Co-occurrence distance (cooccur)","title":"kmed: Distance-Based K-Medoids","text":"co-ocurrence distance (Ahmad Dey 2007; Harikumar PV 2015) can calculated via cooccur function. calculate distance objects, distribution variables taken consideration. Compared simple matching distance, co-occurrence distance redefines score match mis-match categories unnecessary 0 1, respectively. Due relying distribution inclusion variables, co-occurence distance data set single variable absent. co-occurrence distance bin data set show co-occurrence distance calculated, distance objects 1 2 presented. Step 1 Creating cross tabulations Step 2 Calculating column proportions cross tabulation Step 3 Finding maximum values row proportion Step 4 Defining scores variable score obtained summation maximum value subtracted divided constant. constant value depending number inclusion variables. bin data set, constant 1 x y variables depended one variable, .e.Â x depends distribution y y relies distribution x. can implied score mis-match categories 0.5 0.67 x y variables, respectively. Note score match categories alwalys 0. Thus, distance objects 1 2 0+0.6666667 = 0.6666667 objects 1 3 0.5+0.6666667 = 1.1666667 (Back Intoduction)","code":"#calculate co-occurrence distance cooccur(bin) ##           1         2         3         4 ## 1 0.0000000 0.6666667 0.3333333 0.3333333 ## 2 0.6666667 0.0000000 0.3333333 0.3333333 ## 3 0.3333333 0.3333333 0.0000000 0.0000000 ## 4 0.3333333 0.3333333 0.0000000 0.0000000 bin ##   x y ## 1 1 2 ## 2 2 1 ## 3 1 1 ## 4 1 1 #cross tabulation to define score in the y variable (tab.y <- table(bin[,'x'], bin[,'y'])) ##     ##     1 2 ##   1 2 1 ##   2 1 0 #cross tabulation to define score in the x variable (tab.x <- table(bin[,'y'], bin[,'x'])) ##     ##     1 2 ##   1 2 1 ##   2 1 0 #proportion in the y variable (prop.y <- apply(tab.y, 2, function(x) x/sum(x))) ##     ##             1 2 ##   1 0.6666667 1 ##   2 0.3333333 0 #proportion in the x variable (prop.x <- apply(tab.x, 2, function(x) x/sum(x))) ##     ##             1 2 ##   1 0.6666667 1 ##   2 0.3333333 0 #maximum proportion in the y variable (max.y <- apply(prop.y, 2, function(x) max(x))) ##         1         2  ## 0.6666667 1.0000000 #maximum proportion in the x variable (max.x <- apply(prop.x, 2, function(x) max(x))) ##         1         2  ## 0.6666667 1.0000000 #score mis-match in the y variable (sum(max.y) - 1)/1 ## [1] 0.6666667 #score mis-match in the x variable (sum(max.x) - 1)/1 ## [1] 0.6666667"},{"path":"/articles/kmedoid.html","id":"c--mixed-variables-distmix","dir":"Articles","previous_headings":"2. Distance Computation","what":"2.C. Mixed variables (distmix)","title":"kmed: Distance-Based K-Medoids","text":"six available distance methods mixed variable data set. distmix function calculates mixed variable distance requires column id class variables. mixdata data set generated describe method distmix function.","code":"cat <- matrix(c(1, 3, 2, 1, 3, 1, 2, 2), 4, 2) mixdata <- cbind(iris[c(1:2, 51:52),3:4], bin, cat) rownames(mixdata) <- 1:nrow(mixdata) colnames(mixdata) <- c(paste(c(\"num\"), 1:2, sep = \"\"),                         paste(c(\"bin\"), 1:2, sep = \"\"),                         paste(c(\"cat\"), 1:2, sep = \"\"))"},{"path":"/articles/kmedoid.html","id":"gower","dir":"Articles","previous_headings":"2. Distance Computation > 2.C. Mixed variables (distmix)","what":"2.C.1 Gower (method = \"gower\")","title":"kmed: Distance-Based K-Medoids","text":"method = \"gower\" distmix function calculates Gower (1971) distance. original Gower distance allows missing values, allowed distmix function. Gower distance mixdata data set example, distance objects 3 4 presented. range numerical variables necessary. Gower distance calculates Gower similarity first. Gower similarity, mis-match categories binary/ categorical variables scored 0 match categories 1. Meanwhile, numerical variables, 1 subtracted ratio absolute difference range. , Gower similarity can weighted number variables. Thus, Gower similarity objects 3 4 Gower distance obtained subtracting 1 Gower similarity. distance objects 3 4 (Back Intoduction)","code":"mixdata ##   num1 num2 bin1 bin2 cat1 cat2 ## 1  1.4  0.2    1    2    1    3 ## 2  1.4  0.2    2    1    3    1 ## 3  4.7  1.4    1    1    2    2 ## 4  4.5  1.5    1    1    1    2 #calculate the Gower distance distmix(mixdata, method = \"gower\", idnum = 1:2, idbin = 3:4, idcat = 5:6) ##           1         2         3         4 ## 1 0.0000000 0.6666667 0.8205128 0.6565657 ## 2 0.6666667 0.0000000 0.8205128 0.8232323 ## 3 0.8205128 0.8205128 0.0000000 0.1895882 ## 4 0.6565657 0.8232323 0.1895882 0.0000000 #extract the range of each numerical variable apply(mixdata[,1:2], 2, function(x) max(x)-min(x)) ## num1 num2  ##  3.3  1.3 #the Gower similarity (gowsim <- ((1-abs(4.7-4.5)/3.3) + (1-abs(1.4-1.5)/1.3) + 1 + 1 + 0 + 1)/ 6 ) ## [1] 0.8104118 #the Gower distance 1 - gowsim ## [1] 0.1895882"},{"path":"/articles/kmedoid.html","id":"wishart","dir":"Articles","previous_headings":"2. Distance Computation > 2.C. Mixed variables (distmix)","what":"2.C.2 Wishart (method = \"wishart\")","title":"kmed: Distance-Based K-Medoids","text":"Wishart (2003) distance can calculated via method = \"wishart\". Although allows missing values, illegitimate distmix function. Wishart distance mixdata calculate Wishart distance, variance numerical variable required. weighs squared difference numerical variable. Meanwhile, mis-match categories binary/ categorical variables scored 1 match categories 0. , score variables added squared rooted. Thus, distance objects 3 4 (Back Intoduction)","code":"#calculate the Wishart distance distmix(mixdata, method = \"wishart\", idnum = 1:2, idbin = 3:4, idcat = 5:6) ##           1         2         3         4 ## 1 0.0000000 0.8164966 1.2206686 1.1578998 ## 2 0.8164966 0.0000000 1.2206686 1.2277616 ## 3 1.2206686 1.2206686 0.0000000 0.4144946 ## 4 1.1578998 1.2277616 0.4144946 0.0000000 #extract the variance of each numerical variable apply(mixdata[,1:2], 2, function(x) var(x)) ##   num1   num2  ## 3.4200 0.5225 wish <- (((4.7-4.5)^2/3.42) + ((1.4-1.5)^2/0.5225) + 0 + 0 + 1 + 0)/ 6  #the Wishart distance sqrt(wish) ## [1] 0.4144946"},{"path":"/articles/kmedoid.html","id":"podani","dir":"Articles","previous_headings":"2. Distance Computation > 2.C. Mixed variables (distmix)","what":"2.C.3 Podani (method = \"podani\")","title":"kmed: Distance-Based K-Medoids","text":"method = \"podani\" distmix function calculates Podani (1999) distance. Similar Gower Wishart distances, allows missing values, yet allowed distmix function. Podani distance mixdata Podani Wishart distances similar. different denumerator numerical variables. Instead variance, Podani distance applies squared range numerical variable. Unlike Gower Podani distances, number variables weight absent Podani distance. Hence, distance objects 3 4 based data (Back Intoduction)","code":"#calculate Podani distance distmix(mixdata, method = \"podani\", idnum = 1:2, idbin = 3:4, idcat = 5:6) ##          1        2        3        4 ## 1 0.000000 2.000000 2.202742 1.970396 ## 2 2.000000 0.000000 2.202742 2.209629 ## 3 2.202742 2.202742 0.000000 1.004784 ## 4 1.970396 2.209629 1.004784 0.000000 poda <- ((4.7-4.5)^2/3.3^2) + ((1.4-1.5)^2/1.3^2) + 0 + 0 + 1 + 0  #the Podani distance sqrt(poda) ## [1] 1.004784 ##   num1 num2 bin1 bin2 cat1 cat2 ## 3  4.7  1.4    1    1    2    2 ## 4  4.5  1.5    1    1    1    2"},{"path":"/articles/kmedoid.html","id":"huang","dir":"Articles","previous_headings":"2. Distance Computation > 2.C. Mixed variables (distmix)","what":"2.C.4 Huang (method = \"huang\")","title":"kmed: Distance-Based K-Medoids","text":"method = \"huang\" distmix function calculates Huang (1997) distance. Huang distance mixdata data set average standard deviation numerical variables required calculate Huang distance. measure weighs binary/ categorical variables. squared difference numerical variables calculated, mis-match categories scored 1 match categories 0 binary/ categorical variables. Thus, distance objects 3 4 (Back Intoduction)","code":"#calculate the Huang distance distmix(mixdata, method = \"huang\", idnum = 1:2, idbin = 3:4, idcat = 5:6) ##           1         2         3         4 ## 1  0.000000  5.144332 16.188249 13.872166 ## 2  5.144332  0.000000 16.188249 15.158249 ## 3 16.188249 16.188249  0.000000  1.336083 ## 4 13.872166 15.158249  1.336083  0.000000 #find the average standard deviation of the numerical variables mean(apply(mixdata[,1:2], 2, function(x) sd(x))) ## [1] 1.286083 (4.7-4.5)^2 + (1.4-1.5)^2 + 1.286083*(0 + 0) + 1.286083*(1 + 0) ## [1] 1.336083"},{"path":"/articles/kmedoid.html","id":"harikumar","dir":"Articles","previous_headings":"2. Distance Computation > 2.C. Mixed variables (distmix)","what":"2.C.5 Harikumar and PV (method = \"harikumar\")","title":"kmed: Distance-Based K-Medoids","text":"Harikumar PV (2015) distance can calculated via method = \"harikumar\". Harikumar PV distance mixdata Harikumar PV distance requires absolute difference numerical variables unweighted simple matching, .e.Â Hamming distance, binary variables. categorical variables, applies co-occurrence distance. co-occurence distance categorical variables (manual calculation see co-occurrence subsection) Hence, distance objects 1 3 data (Back Intoduction)","code":"#calculate Harikumar-PV distance distmix(mixdata, method = \"harikumar\", idnum = 1:2, idbin = 3:4, idcat = 5:6) ##     1   2   3   4 ## 1 0.0 4.0 6.5 5.9 ## 2 4.0 0.0 7.5 7.4 ## 3 6.5 7.5 0.0 0.8 ## 4 5.9 7.4 0.8 0.0 cooccur(mixdata[,5:6]) ##     1 2   3   4 ## 1 0.0 2 1.0 0.5 ## 2 2.0 0 2.0 2.0 ## 3 1.0 2 0.0 0.5 ## 4 0.5 2 0.5 0.0 abs(4.7-4.5) + abs(1.4-1.5) + (0 + 0) + (0.5) ## [1] 0.8 ##   num1 num2 bin1 bin2 cat1 cat2 ## 3  4.7  1.4    1    1    2    2 ## 4  4.5  1.5    1    1    1    2"},{"path":"/articles/kmedoid.html","id":"ahmad","dir":"Articles","previous_headings":"2. Distance Computation > 2.C. Mixed variables (distmix)","what":"2.C.6 Ahmad and Dey (method = \"ahmad\")","title":"kmed: Distance-Based K-Medoids","text":"method = \"ahmad\" distmix function calculates Ahmad Dey (2007) distance. Ahmad Dey distance mixdata data set Ahmad dey distance requires squared difference numerical variables co-occurrence distance binary categorical variables. co-occurrence distance mixdata data set Thus, distance objects 2 3 based data (Back Intoduction)","code":"#calculate Ahmad-Dey distance distmix(mixdata, method = \"ahmad\", idnum = 1:2, idbin = 3:4, idcat = 5:6) ##          1            2            3            4 ## 1  0.00000 1.074383e+01 1.458000e+01 1.266111e+01 ## 2 10.74383 2.191280e-32 1.678679e+01 1.648827e+01 ## 3 14.58000 1.678679e+01 2.191280e-32 1.611111e-01 ## 4 12.66111 1.648827e+01 1.611111e-01 2.191280e-32 cooccur(mixdata[,3:6]) ##          1             2             3             4 ## 1 0.000000  3.277778e+00  1.500000e+00  1.166667e+00 ## 2 3.277778 -1.480297e-16  2.111111e+00  2.277778e+00 ## 3 1.500000  2.111111e+00 -1.480297e-16  3.333333e-01 ## 4 1.166667  2.277778e+00  3.333333e-01 -1.480297e-16 (1.4-4.7)^2 + (0.2-1.4)^2 + (2)^2 ## [1] 16.33 ##   num1 num2 bin1 bin2 cat1 cat2 ## 2  1.4  0.2    2    1    3    1 ## 3  4.7  1.4    1    1    2    2"},{"path":"/articles/kmedoid.html","id":"k-medoids-algorithms","dir":"Articles","previous_headings":"","what":"3. K-medoids algorithms","title":"kmed: Distance-Based K-Medoids","text":"k-medoids algorithms available package. simple fast k-medoids (fastkmed), k-medoids, ranked k-medoids (rankkmed), increasing number clusters k-medoids (inckmed). algorithms list results, namely cluster membership, id medoids, distance objects medoid. section, algorithms applied iris data set applying mrw distance (see Manhattan weighted range). number clusters data set 3.","code":""},{"path":"/articles/kmedoid.html","id":"sfkm","dir":"Articles","previous_headings":"3. K-medoids algorithms","what":"3.A. Simple and fast k-medoids algorithm (fastkmed)","title":"kmed: Distance-Based K-Medoids","text":"simple fast k-medoid (SFKM) algorithm proposed Park Jun (2009). fastkmed function runs algorithm cluster objects. compulsory inputs distance matrix distance object number clusters. Hence, SFKM algorithm iris data set , classification table can obtained. Applying SFKM algorithm iris data set Manhattan weighted range, misclassification rate (Back Intoduction)","code":"#run the sfkm algorihtm on iris data set with mrw distance (sfkm <- fastkmed(mrwdist, ncluster = 3, iterate = 50)) ## $cluster ##   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  ##   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1  ##  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  ##   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1  ##  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  ##   1   1   1   1   1   1   1   1   1   1   3   3   3   2   3   2   3   2   2   2  ##  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  ##   2   2   2   2   2   3   2   2   2   2   3   2   2   2   2   3   3   3   2   2  ##  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100  ##   2   2   2   2   2   2   3   2   2   2   2   2   2   2   2   2   2   2   2   2  ## 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120  ##   3   3   3   3   3   3   2   3   3   3   3   3   3   3   3   3   3   3   3   2  ## 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140  ##   3   3   3   3   3   3   3   3   3   3   3   3   3   3   2   3   3   3   3   3  ## 141 142 143 144 145 146 147 148 149 150  ##   3   3   3   3   3   3   3   3   3   3  ##  ## $medoid ## [1]   8  95 148 ##  ## $minimum_distance ## [1] 45.76718 (sfkmtable <- table(sfkm$cluster, iris[,5])) ##     ##     setosa versicolor virginica ##   1     50          0         0 ##   2      0         39         3 ##   3      0         11        47 (3+11)/sum(sfkmtable) ## [1] 0.09333333"},{"path":"/articles/kmedoid.html","id":"km","dir":"Articles","previous_headings":"3. K-medoids algorithms","what":"3.B. K-medoids algorithm","title":"kmed: Distance-Based K-Medoids","text":"Reynolds et al. (2006) proposed k-medoids (KM) algorithm. similar SFKM fastkmed can applied. difference initial medoid selection KM selects initial medoid randomly. Thus, KM algorithm iris data set setting init classification table KM algorithm misclassification rate Compared SFKM algorithm, 9.33% misclassification, misclassification KM algorithm slightly better (8%). (Back Intoduction)","code":"#set the initial medoids set.seed(1) (kminit <- sample(1:nrow(iris), 3)) ## [1]  68 129  43 #run the km algorihtm on iris data set with mrw distance (km <- fastkmed(mrwdist, ncluster = 3, iterate = 50, init = kminit)) ## $cluster ##   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  ##   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3  ##  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  ##   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3  ##  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  ##   3   3   3   3   3   3   3   3   3   3   2   2   2   1   1   1   2   1   1   1  ##  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  ##   1   1   1   1   1   2   1   1   1   1   2   1   1   1   1   2   1   2   1   1  ##  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100  ##   1   1   1   1   1   1   2   1   1   1   1   1   1   1   1   1   1   1   1   1  ## 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120  ##   2   2   2   2   2   2   1   2   2   2   2   2   2   2   2   2   2   2   2   1  ## 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140  ##   2   2   2   2   2   2   2   2   2   2   2   2   2   2   1   2   2   2   2   2  ## 141 142 143 144 145 146 147 148 149 150  ##   2   2   2   2   2   2   2   2   2   2  ##  ## $medoid ## [1] 100 148   8 ##  ## $minimum_distance ## [1] 48.8411 (kmtable <- table(km$cluster, iris[,5])) ##     ##     setosa versicolor virginica ##   1      0         41         3 ##   2      0          9        47 ##   3     50          0         0 (3+9)/sum(kmtable) ## [1] 0.08"},{"path":"/articles/kmedoid.html","id":"rkm","dir":"Articles","previous_headings":"3. K-medoids algorithms","what":"3.C. Rank k-medoids algorithm (rankkmed)","title":"kmed: Distance-Based K-Medoids","text":"rank k-medoids (RKM) proposed Zadegan, Mirzaie, Sadoughi (2013). rankkmed function runs RKM algorithm. m argument introduced calculate hostility score. m indicates many closest objects selected. selected objects initial medoids RKM randomly assigned. RKM algorithm iris data set setting m = 10 , classification table attained misclassification proportion 4% misclassification rate, RKM algorithm best among three previous algorithms. (Back Intoduction)","code":"#run the rkm algorihtm on iris data set with mrw distance and m = 10 (rkm <- rankkmed(mrwdist, ncluster = 3, m = 10, iterate = 50)) ## $cluster ##   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  ##   2   2   1   1   2   2   1   2   1   2   2   2   1   1   2   2   2   2   2   2  ##  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  ##   2   2   1   2   2   2   2   2   2   1   1   2   2   2   2   2   2   2   1   2  ##  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  ##   2   1   1   2   2   1   2   1   2   2   3   3   3   3   3   3   3   3   3   3  ##  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  ##   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3  ##  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100  ##   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3  ## 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120  ##   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3  ## 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140  ##   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3  ## 141 142 143 144 145 146 147 148 149 150  ##   3   3   3   3   3   3   3   3   3   3  ##  ## $medoid ## [1] \"3\"  \"50\" \"55\" ##  ## $minimum_distance ## [1] 65.20221 (rkmtable <- table(rkm$cluster, iris[,5])) ##     ##     setosa versicolor virginica ##   1     14          0         0 ##   2     36          0         0 ##   3      0         50        50 (3+3)/sum(rkmtable) ## [1] 0.04"},{"path":"/articles/kmedoid.html","id":"inckm","dir":"Articles","previous_headings":"3. K-medoids algorithms","what":"3.D. Increasing number of clusters k-medoids algorithm (inckmed)","title":"kmed: Distance-Based K-Medoids","text":"Yu et al. (2018) proposed increasing number clusters k-medoids (INCKM) algorithm. algorithm implemented inckmed function. alpha argument indicates stretch factor select initial medoids. SFKM, KM INCKM similar algorithm different way select initial medoids. INCKM algorithm iris data set alpha = 1.1 , classification table can attained. misclassification rate algorithm 8% misclassification rate RKM algorithm performs best among four algorithms iris data set mrw distance. (Back Intoduction)","code":"#run the inckm algorihtm on iris data set with mrw distance and alpha = 1.2 (inckm <- inckmed(mrwdist, ncluster = 3, alpha = 1.1, iterate = 50)) ## $cluster ##   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  ##   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1  ##  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  ##   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1  ##  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  ##   1   1   1   1   1   1   1   1   1   1   3   2   3   2   2   2   2   2   2   2  ##  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  ##   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   3   2   2  ##  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100  ##   2   2   2   2   2   2   3   2   2   2   2   2   2   2   2   2   2   2   2   2  ## 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120  ##   3   2   3   3   3   3   2   3   3   3   3   3   3   2   3   3   3   3   3   2  ## 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140  ##   3   2   3   2   3   3   2   3   3   3   3   3   3   2   2   3   3   3   2   3  ## 141 142 143 144 145 146 147 148 149 150  ##   3   3   2   3   3   3   3   3   3   3  ##  ## $medoid ## [1]   8  56 113 ##  ## $minimum_distance ## [1] 45.44091 (inckmtable <- table(inckm$cluster, iris[,5])) ##     ##     setosa versicolor virginica ##   1     50          0         0 ##   2      0         46        11 ##   3      0          4        39 (9+3)/sum(inckmtable) ## [1] 0.08"},{"path":"/articles/kmedoid.html","id":"skm","dir":"Articles","previous_headings":"3. K-medoids algorithms","what":"3.E. Simple k-medoids algorithm (skm)","title":"kmed: Distance-Based K-Medoids","text":"simple k-medoid (SKM) algorithm proposed Budiaji Leisch (2019). skm function runs algorithm cluster objects. compulsory inputs distance matrix distance object number clusters. Hence, SKM algorithm iris data set , classification table can obtained. Applying SKM algorithm iris data set Manhattan weighted range, misclassification rate (Back Intoduction)","code":"#run the sfkm algorihtm on iris data set with mrw distance (simplekm <- skm(mrwdist, ncluster = 3, seeding = 50)) ## $cluster ##   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  ##   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1  ##  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  ##   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1  ##  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  ##   1   1   1   1   1   1   1   1   1   1   2   3   2   3   3   3   3   3   3   3  ##  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  ##   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   2   3   3  ##  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100  ##   3   3   3   3   3   3   2   3   3   3   3   3   3   3   3   3   3   3   3   3  ## 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120  ##   2   3   2   2   2   2   3   2   2   2   2   2   2   3   2   2   2   2   2   3  ## 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140  ##   2   3   2   3   2   2   3   2   2   2   2   2   2   3   3   2   2   2   3   2  ## 141 142 143 144 145 146 147 148 149 150  ##   2   2   3   2   2   2   2   2   2   2  ##  ## $medoid ## [1]   8 113  56 ##  ## $minimum_distance ## [1] 45.44091 (simpletable <- table(simplekm$cluster, iris[,5])) ##     ##     setosa versicolor virginica ##   1     50          0         0 ##   2      0          4        39 ##   3      0         46        11 (4+11)/sum(simpletable) ## [1] 0.1"},{"path":"/articles/kmedoid.html","id":"cluster-validation","dir":"Articles","previous_headings":"","what":"4. Cluster validation","title":"kmed: Distance-Based K-Medoids","text":"clustering algorithm result validated. two types validation implemented kmed package. internal relative criteria validations.","code":""},{"path":[]},{"path":"/articles/kmedoid.html","id":"sil","dir":"Articles","previous_headings":"4. Cluster validation","what":"4.A.1. Silhouette (sil)","title":"kmed: Distance-Based K-Medoids","text":"Rousseeuw (1987) proposed silhouette index internal measure validation. based average distance objects within cluster nearest cluster. sil function calculates silhouette index clustering result. arguments distance matrix distance object, id medoids, cluster membership. produce list silhouette indices sihouette plots. silhouette index plot best clustering result iris data set via RKM presented. silhouette index object can obtained , plot presented  (Back Intoduction)","code":"#calculate silhouette of the RKM result of iris data set  siliris <- sil(mrwdist, rkm$medoid, rkm$cluster,                       title = \"Silhouette plot of Iris data set via RKM\") #silhouette indices of objects 49 to 52 siliris$result[c(49:52),] ##    silhouette cluster ## 49 0.49986165       2 ## 50 0.01977318       2 ## 51 0.59663837       3 ## 52 0.61488150       3 siliris$plot"},{"path":"/articles/kmedoid.html","id":"csv","dir":"Articles","previous_headings":"4. Cluster validation","what":"4.A.2. Centroid-based shadow value (csv)","title":"kmed: Distance-Based K-Medoids","text":"way measure internal validation corresponding plot presenting centroid-based shadow value (Leisch 2010). csv function calculates plots centroid-base shadow value object, based first second closest medoids. centroid original version csv replaced medoids csv function adapt k-medoids algorithm. required arguments csv function identical silhouette (sil) function. Thus, shadow value plot best clustering result iris data set via RKM can obtained centroid-based shadow values objects 49 52, instance, presented centroid-based shadow value plot also produced.  (Back Intoduction)","code":"#calculate centroid-base shadow value of the RKM result of iris data set  csviris <- csv(mrwdist, rkm$medoid, rkm$cluster,                       title = \"CSV plot of Iris data set via RKM\") #shadow values of objects 49 to 52 csviris$result[c(49:52),] ##          csv cluster ## 49 0.7899687       2 ## 50 0.0000000       2 ## 51 0.3604380       3 ## 52 0.2473829       3 csviris$plot"},{"path":"/articles/kmedoid.html","id":"msv","dir":"Articles","previous_headings":"4. Cluster validation","what":"4.A.3. Medoid-based shadow value (msv)","title":"kmed: Distance-Based K-Medoids","text":"way measure internal validation combining silhoutte csv properties medoid-based shadow value (msv) (Budiaji 2019). msv function calculates plots medoid-based shadow value object, based first second closest medoids. required arguments msv function identical centroid-based shadow value (csv) function. Thus, medoid-based shadow value plot best clustering result iris data set via RKM can obtained medoid-based shadow values objects 49 52, instance, presented shadow value plot also produced.  (Back Intoduction)","code":"#calculate medoid-based shadow value of the RKM result of iris data set  msviris <- msv(mrwdist, rkm$medoid, rkm$cluster,                       title = \"MSV plot of Iris data set via RKM\") #Medoid-based shadow values of objects 49 to 52 msviris$result[c(49:52),] ##          msv cluster ## 49 0.3471503       2 ## 50 1.0000000       2 ## 51 0.7801620       3 ## 52 0.8588494       3 msviris$plot"},{"path":"/articles/kmedoid.html","id":"boot","dir":"Articles","previous_headings":"4. Cluster validation","what":"4.B. Relative criteria","title":"kmed: Distance-Based K-Medoids","text":"relative criteria evaluate clustering algorithm result applying re-sampling strategy. Thus, bootstrap strategy can applied. expected result cluster bootstraping robust replications (Dolnicar Leisch 2010). three steps validate cluster result via boostraping strategy.","code":""},{"path":"/articles/kmedoid.html","id":"stp1","dir":"Articles","previous_headings":"4. Cluster validation > 4.B. Relative criteria","what":"Step 1 Creating a matrix of bootstrap replicates","title":"kmed: Distance-Based K-Medoids","text":"create matrix bootstrap replicates, clustboot function can applied. five arguments clustboot function algorithm argument important. algorithm argument argument clustering algorithm user wants evaluate. function. RKM iris data set validated, instance, RKM function, required input algorithm argument, function created, two input arguments. x (distance matrix) nclust (number clusters). output, hand, vector cluster membership (res$cluster). Thus, matrix bootstrap replicates can produced objects 1 4 first fifth replications rkmbootstrap matrix bootrstrap replicates dimension 150 x 50, .e.Â n x b, n number objects b number bootstrap replicates. Note default evaluated algorithm SFKM algorithm user ignores algorithm argument, matrix bootstrap replicates can still produced. However, misleads evaluate userâs algorithm.","code":"#The RKM function for an argument input rkmfunc <- function(x, nclust) {   res <- rankkmed(x, nclust, m = 10, iterate = 50)   return(res$cluster) } #The RKM algorthim evaluation by inputing the rkmfunc function #in the algorithm argument rkmbootstrap <- clustboot(mrwdist, nclust=3, nboot=50, algorithm = rkmfunc) rkmbootstrap[1:4,1:5] ##      [,1] [,2] [,3] [,4] [,5] ## [1,]    1    1    1    0    1 ## [2,]    1    1    1    1    0 ## [3,]    0    1    0    1    0 ## [4,]    0    1    0    0    2"},{"path":"/articles/kmedoid.html","id":"stp2","dir":"Articles","previous_headings":"4. Cluster validation > 4.B. Relative criteria","what":"Step 2 Transforming the bootstrap matrix into a consensus matrix","title":"kmed: Distance-Based K-Medoids","text":"matrix bootstrap replicates produced clustboot step 1 can transformed consensus matrix dimension n x n via consensusmatrix function. element consensus matrix row dan column j agreement value objects j cluster taken sample time (Monti et al. 2003). However, requires algorithm order objects way objects cluster close . consensusmatrix function reorder argument comply task. similar algorithm argument clustboot function step 1 reorder function two arguments vector output. Transforming rkmbootstrap consensus matrix via ward linkage algorithm oder objects, example, can obtained first fourth rows columns can displayed ","code":"#The ward function to order the objects in the consensus matrix wardorder <- function(x, nclust) {   res <- hclust(as.dist(x), method = \"ward.D2\")   member <- cutree(res, nclust)   return(member) } consensusrkm <- consensusmatrix(rkmbootstrap, nclust = 3, wardorder) consensusrkm[c(1:4),c(1:4)] ##           1         1         1         1 ## 1 1.0000000 0.8823529 0.7333333 0.8421053 ## 1 0.8823529 1.0000000 1.0000000 0.9444444 ## 1 0.7333333 1.0000000 1.0000000 0.9473684 ## 1 0.8421053 0.9444444 0.9473684 1.0000000"},{"path":"/articles/kmedoid.html","id":"step-3-visualizing-the-consensus-matrix-in-a-heatmap","dir":"Articles","previous_headings":"4. Cluster validation > 4.B. Relative criteria","what":"Step 3 Visualizing the consensus matrix in a heatmap","title":"kmed: Distance-Based K-Medoids","text":"ordered consensus matrix step 2 can visualized heatmap applying clustheatmap function. agreement indices consensus matrix can transformed via non-linear transformation (Hahsler Hornik 2011). Thus, consensusrkm can visualize  (Back Intoduction)","code":"clustheatmap(consensusrkm, \"Iris data evaluated by the RKM, ordered by Ward linkage\")"},{"path":"/articles/kmedoid.html","id":"cluster-visualization","dir":"Articles","previous_headings":"","what":"5. Cluster visualization","title":"kmed: Distance-Based K-Medoids","text":"cluster visualization clustering result can enhance data structure understanding. biplot marked barplot presented visualize clustering result.","code":""},{"path":"/articles/kmedoid.html","id":"biplot","dir":"Articles","previous_headings":"5. Cluster visualization","what":"A. Biplot","title":"kmed: Distance-Based K-Medoids","text":"pcabiplot function can applied plot clustering result numerical data set. numerical data set converted principle component object via prcomp function. x y axes plot can replaced component principle components. colour objects can adjusted based cluster membership supplying vector membership colobj argument. iris data set can plotted pca biplot colour objects based RKM algorithm result.  second principle component can replaced third principle component.  (Back Intoduction)","code":"#convert the data set into principle component object pcadat <- prcomp(iris[,1:4], scale. = TRUE) #plot the pca with the corresponding RKM clustering result  pcabiplot(pcadat, colobj = rkm$cluster, o.size = 2) pcabiplot(pcadat, y = \"PC3\",colobj = rkm$cluster, o.size = 1.5)"},{"path":"/articles/kmedoid.html","id":"barplotnum","dir":"Articles","previous_headings":"5. Cluster visualization","what":"B. Marked barplot","title":"kmed: Distance-Based K-Medoids","text":"marked barplot proposed Dolnicar Leisch (2014); Leisch (2008) mark indicates significant difference clusterâs mean populationâs mean variable. barplot function creates barplot cluster particular significant level. layout barplot set nc argument. barplot iris data set partitioned RKM algorithm  layout changed 2 columns alpha set 1%, becomes  (Back Intoduction)","code":"barplotnum(iris[,1:4], rkm$cluster, alpha = 0.05) barplotnum(iris[,1:4], rkm$cluster, nc = 2, alpha = 0.01)"},{"path":[]},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Weksi Budiaji. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Budiaji W (2022). kmed: Distance-Based k-Medoids. R package version 0.4.2, https://CRAN.R-project.org/package=kmed.","code":"@Manual{,   title = {kmed: Distance-Based k-Medoids},   author = {Weksi Budiaji},   year = {2022},   note = {R package version 0.4.2},   url = {https://CRAN.R-project.org/package=kmed}, }"},{"path":"/reference/barplotnum.html","id":null,"dir":"Reference","previous_headings":"","what":"Barplot of each cluster for numerical variables data set â barplotnum","title":"Barplot of each cluster for numerical variables data set â barplotnum","text":"function creates barplot cluster result. barplot indicates location dispersion cluster. x-axis barplot variable's mean, y-axis variable's name.","code":""},{"path":"/reference/barplotnum.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Barplot of each cluster for numerical variables data set â barplotnum","text":"","code":"barplotnum(dataori, clust, nc = 1, alpha = 0.05)"},{"path":"/reference/barplotnum.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Barplot of each cluster for numerical variables data set â barplotnum","text":"dataori original data set. clust vector cluster membership (see Details). nc number columns plot cluster (see Details). alpha numeric number set significant level (0 0.2).","code":""},{"path":"/reference/barplotnum.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Barplot of each cluster for numerical variables data set â barplotnum","text":"Function returns barplot.","code":""},{"path":"/reference/barplotnum.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Barplot of each cluster for numerical variables data set â barplotnum","text":"marked barplot markers added, .e. significant test, population mean (numerical) variable. significance test applies t-test population's mean cluster's mean every variable. alpha set 0 20%. population mean differs cluster's mean, bar shade barplot also differs. clust vector length equal number objects (n), function error otherwise. nc controls layout (grid) plot. nc = 1, plot cluster placed column. number clusters 6 nc = 2, example, plot layout 3-row 2-column grids.","code":""},{"path":"/reference/barplotnum.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Barplot of each cluster for numerical variables data set â barplotnum","text":"Leisch, F. (2008). Handbook Data Visualization, Chapter Visualizing cluster analysis finite mixture models, pp. 561-587. Springer Handbooks Computational Statistics. Springer Verlag. Dolnicar, S. F. Leisch (2014). Using graphical statistics better understand market segmentation solutions. International Journal Market Research 56, 207-230.","code":""},{"path":"/reference/barplotnum.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Barplot of each cluster for numerical variables data set â barplotnum","text":"Weksi Budiaji  Contact: budiaji@untirta.ac.id","code":""},{"path":"/reference/barplotnum.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Barplot of each cluster for numerical variables data set â barplotnum","text":"","code":"dat <- iris[,1:4] memb <- cutree(hclust(dist(dat)),3) barplotnum(dat, memb)  barplotnum(dat, memb, 2)"},{"path":"/reference/clust4.html","id":null,"dir":"Reference","previous_headings":"","what":"4-clustered data set â clust4","title":"4-clustered data set â clust4","text":"dataset containing two variables 300 objects class memberships generated clusterGeneration package.","code":""},{"path":"/reference/clust4.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"4-clustered data set â clust4","text":"","code":"clust4"},{"path":"/reference/clust4.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"4-clustered data set â clust4","text":"data frame 300 rows 3 variables: x1 X1. x2 X2. class Class membership.","code":""},{"path":"/reference/clust4.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"4-clustered data set â clust4","text":"Data generated via genRandomClust function clusterGeneration package. code generate data set set.seed(2016) randclust <- clusterGeneration::genRandomClust(4, sepVal = 0.001, numNonNoisy = 2, numReplicate = 1, clustszind = 3, clustSizes = .numeric(table(sample(1:4, 300, replace = TRUE))), outputDatFlag=FALSE, outputLogFlag=FALSE, outputEmpirical=FALSE, outputInfo=FALSE) clust4 <- .data.frame(randclust$datList$test_1) clust4$class <- randclust$memList$test_1","code":""},{"path":"/reference/clust4.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"4-clustered data set â clust4","text":"Qiu, W., H. Joe. 2015. ClusterGeneration: Random Cluster Generation (Specified Degree Separation). Qiu, W., H. Joe. 2006a. Generation Random Clusters Specified Degree Separation. Journal Classification 23 pp. 315-34. Qiu, W., H. Joe. 2006b. Separation Index Partial Membership Clustering. Computational Statistics Data Analysis 50 pp. 585-603.","code":""},{"path":"/reference/clust5.html","id":null,"dir":"Reference","previous_headings":"","what":"5-clustered data set â clust5","title":"5-clustered data set â clust5","text":"dataset containing two variables 800 objects class memberships generated clusterGeneration package.","code":""},{"path":"/reference/clust5.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"5-clustered data set â clust5","text":"","code":"clust5"},{"path":"/reference/clust5.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"5-clustered data set â clust5","text":"data frame 800 rows 3 variables: x1 X1. x2 X2. class Class membership.","code":""},{"path":"/reference/clust5.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"5-clustered data set â clust5","text":"Data generated via genRandomClust function clusterGeneration package. code generate data set set.seed(2016) randclust <- clusterGeneration::genRandomClust(5, sepVal = 0.2, numNonNoisy = 2, numReplicate = 1, clustszind = 3, clustSizes = .numeric(table(sample(1:5, 800, replace = TRUE))), outputDatFlag=FALSE, outputLogFlag=FALSE, outputEmpirical=FALSE, outputInfo=FALSE) clust5 <- .data.frame(randclust$datList$test_1) clust5$class <- randclust$memList$test_1","code":""},{"path":"/reference/clust5.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"5-clustered data set â clust5","text":"Qiu, W., H. Joe. 2015. ClusterGeneration: Random Cluster Generation (Specified Degree Separation). Qiu, W., H. Joe. 2006a. Generation Random Clusters Specified Degree Separation. Journal Classification 23 pp. 315-34. Qiu, W., H. Joe. 2006b. Separation Index Partial Membership Clustering. Computational Statistics Data Analysis 50 pp. 585-603.","code":""},{"path":"/reference/clustboot.html","id":null,"dir":"Reference","previous_headings":"","what":"Bootstrap replications for clustering alorithm â clustboot","title":"Bootstrap replications for clustering alorithm â clustboot","text":"function bootstrap replications clustering algorithm. hard clustering algorithm valid.","code":""},{"path":"/reference/clustboot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bootstrap replications for clustering alorithm â clustboot","text":"","code":"clustboot(distdata, nclust = 2, algorithm = fastclust, nboot = 25, diss = TRUE)"},{"path":"/reference/clustboot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bootstrap replications for clustering alorithm â clustboot","text":"distdata distance matrix (n x n)/ dist object data frame. nclust number clusters. algorithm clustering algorithm function (see Details). nboot number bootstrap replicates. diss logical distdata distance matrix/ object data frame.","code":""},{"path":"/reference/clustboot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bootstrap replications for clustering alorithm â clustboot","text":"Function returns matrix bootstrap replicates dimension n x b, n number objects b number bootstrap replicates.","code":""},{"path":"/reference/clustboot.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bootstrap replications for clustering alorithm â clustboot","text":"function obtain bootstrap evaluation cluster results. algorithm argument function function two input arguments. two input arguments distance matrix/ object data frame, number clusters. output vector cluster memberships. default algorithm fastclust applying fastkmed function. code fastclust fastclust <- function(x, nclust) { res <- fastkmed(x, nclust, iterate = 50) return(res$cluster) } examples, see Examples. applies ward kmeans algorithms. kmeans applied, example, diss set FALSE input kmclust clustboot data frame instead distance.","code":""},{"path":"/reference/clustboot.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Bootstrap replications for clustering alorithm â clustboot","text":"Dolnicar, S. Leisch, F. 2010. Evaluation structure reproducibility cluster solutions using bootstrap. Marketing Letters 21 pp. 83-101.","code":""},{"path":"/reference/clustboot.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Bootstrap replications for clustering alorithm â clustboot","text":"Weksi Budiaji  Contact: budiaji@untirta.ac.id","code":""},{"path":"/reference/clustboot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bootstrap replications for clustering alorithm â clustboot","text":"","code":"num <- as.matrix(iris[,1:4]) mrwdist <- distNumeric(num, num, method = \"mrw\") ward.D2 <- function(x, nclust) { res <- hclust(as.dist(x), method = \"ward.D2\") member <- cutree(res, nclust) return(member) } kmclust <- function(x, nclust) { res <- kmeans(x, nclust) return(res$cluster) } irisfast <- clustboot(mrwdist, nclust=3, nboot=7) head(irisfast) #>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] #> [1,]    3    1    0    0    1    1    0 #> [2,]    3    1    1    0    0    1    0 #> [3,]    3    1    1    0    0    0    2 #> [4,]    3    1    0    3    0    1    2 #> [5,]    3    0    1    3    0    0    2 #> [6,]    3    1    1    3    0    1    0 irisward <- clustboot(mrwdist, nclust=3, algorithm = ward.D2, nboot=7) head(irisward) #>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] #> [1,]    1    1    1    1    0    0    1 #> [2,]    1    1    0    1    0    0    0 #> [3,]    1    1    1    1    1    1    0 #> [4,]    1    1    1    1    0    1    1 #> [5,]    1    0    0    1    1    1    0 #> [6,]    0    0    1    0    0    1    1 iriskmeans <- clustboot(num, nclust=3, algorithm = kmclust, nboot=7, diss = FALSE) head(iriskmeans) #>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] #> [1,]    0    2    2    0    3    3    2 #> [2,]    2    1    0    0    0    3    0 #> [3,]    0    0    0    3    3    0    0 #> [4,]    0    1    2    3    3    3    2 #> [5,]    2    2    0    3    0    3    2 #> [6,]    0    0    2    0    3    3    2"},{"path":"/reference/clustheatmap.html","id":null,"dir":"Reference","previous_headings":"","what":"Consensus matrix heatmap from A consensus matrix â clustheatmap","title":"Consensus matrix heatmap from A consensus matrix â clustheatmap","text":"function creates consensus matrix heatmap consensus/ agreement matrix. values consensus/ agreement matrix transformed order plot heatmap.","code":""},{"path":"/reference/clustheatmap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Consensus matrix heatmap from A consensus matrix â clustheatmap","text":"","code":"clustheatmap(consmat, title = \"\")"},{"path":"/reference/clustheatmap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Consensus matrix heatmap from A consensus matrix â clustheatmap","text":"consmat matrix consensus/ agreement matrix (see Details). title title plot.","code":""},{"path":"/reference/clustheatmap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Consensus matrix heatmap from A consensus matrix â clustheatmap","text":"Function returns heatmap plot.","code":""},{"path":"/reference/clustheatmap.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Consensus matrix heatmap from A consensus matrix â clustheatmap","text":"function produce consensus matrix heatmap consensus/ agreement matrix. matrix produced consensusmatrix function can directly provided consmat argument. values consensus matrix, , transformed via non-linear transformation applying $$a_{ij}^{trf} = \\frac{a_{ij} - min(a_{..})}{max(a_{..}) - min(a_{..})}$$ \\(a_{ij}\\) value consensus matrix row column j, \\(a_{..}\\) values matrix (\\(\\forall\\)).","code":""},{"path":"/reference/clustheatmap.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Consensus matrix heatmap from A consensus matrix â clustheatmap","text":"Monti, S., P. Tamayo, J. Mesirov, T. Golub. 2003. Consensus clustering: resampling-based method class discovery visualization gene expression microarray data. Machine Learning 52 pp. 91-118. Hahsler, M., Hornik, K., 2011. Dissimilarity plots: visual exploration tool partitional clustering. Journal Computational Graphical Statistics 20(2) pp. 335-354.","code":""},{"path":"/reference/clustheatmap.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Consensus matrix heatmap from A consensus matrix â clustheatmap","text":"Weksi Budiaji  Contact: budiaji@untirta.ac.id","code":""},{"path":"/reference/clustheatmap.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Consensus matrix heatmap from A consensus matrix â clustheatmap","text":"","code":"num <- as.matrix(iris[,1:4]) mrwdist <- distNumeric(num, num, method = \"mrw\") irisfast <- clustboot(mrwdist, nclust=3, nboot=7) complete <- function(x, nclust) { res <- hclust(as.dist(x), method = \"complete\") member <- cutree(res, nclust) return(member) } consensuscomplete <- consensusmatrix(irisfast, nclust = 3, reorder = complete) clustheatmap(consensuscomplete)"},{"path":"/reference/consensusmatrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Consensus matrix from A matrix of bootstrap replicates â consensusmatrix","title":"Consensus matrix from A matrix of bootstrap replicates â consensusmatrix","text":"function creates consensus matrix matrix bootstrap replicates. transforms n x b matrix n x n matrix, n number objects b number bootstrap replicates.","code":""},{"path":"/reference/consensusmatrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Consensus matrix from A matrix of bootstrap replicates â consensusmatrix","text":"","code":"consensusmatrix(bootdata, nclust, reorder = fastclust)"},{"path":"/reference/consensusmatrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Consensus matrix from A matrix of bootstrap replicates â consensusmatrix","text":"bootdata matrix bootstrap replicate (n x b) (see Details). nclust number clusters. reorder distance-based clustering algorithm function (see Details).","code":""},{"path":"/reference/consensusmatrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Consensus matrix from A matrix of bootstrap replicates â consensusmatrix","text":"Function returns consensus/ agreement matrix n x n dimension.","code":""},{"path":"/reference/consensusmatrix.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Consensus matrix from A matrix of bootstrap replicates â consensusmatrix","text":"function obtain consensus matrix matrix bootstrap replicates evaluate clustering result. bootdata argument can supplied directly matrix produced clustboot function. values consensus matrix, , calculated $$a_{ij} = a_{ji} = \\frac{\\#n \\:\\:objects \\:\\:\\:j \\:\\:\\:\\:cluster}{\\#n \\:\\:objects \\:\\:\\:j \\:sampled \\:\\:\\:\\:time}$$ \\(a_{ij}\\) agreement index objects j. Note due agreement objects j equal agreement objects j , consensus matrix symmetric matrix. Meanwhile, reorder argument function reorder objects row column consensus matrix similar objects close . task can solved applying clustering algorithm consensus matrix. reorder consist two input arguments. two input arguments distance matrix/ object number clusters. output vector cluster memberships. Thus, algorihtm can applied reorder argument distance-based algorithm distance input. default reorder fastclust applying fastkmed function. code fastclust fastclust <- function(x, nclust) { res <- fastkmed(x, nclust, iterate = 50) return(res$cluster) } examples, see Examples. applies centroid complete linkage algorithms.","code":""},{"path":"/reference/consensusmatrix.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Consensus matrix from A matrix of bootstrap replicates â consensusmatrix","text":"Monti, S., P. Tamayo, J. Mesirov, T. Golub. 2003. Consensus clustering: resampling-based method class discovery visualization gene expression microarray data. Machine Learning 52 pp. 91-118.","code":""},{"path":"/reference/consensusmatrix.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Consensus matrix from A matrix of bootstrap replicates â consensusmatrix","text":"Weksi Budiaji  Contact: budiaji@untirta.ac.id","code":""},{"path":"/reference/consensusmatrix.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Consensus matrix from A matrix of bootstrap replicates â consensusmatrix","text":"","code":"num <- as.matrix(iris[,1:4]) mrwdist <- distNumeric(num, num, method = \"mrw\") irisfast <- clustboot(mrwdist, nclust=3, nboot=7) consensusfast <- consensusmatrix(irisfast, nclust = 3) centroid <- function(x, nclust) { res <- hclust(as.dist(x), method = \"centroid\") member <- cutree(res, nclust) return(member) } consensuscentroid <- consensusmatrix(irisfast, nclust = 3, reorder = centroid) complete <- function(x, nclust) { res <- hclust(as.dist(x), method = \"complete\") member <- cutree(res, nclust) return(member) } consensuscomplete <- consensusmatrix(irisfast, nclust = 3, reorder = complete) consensusfast[c(1:5,51:55,101:105),c(1:5,51:55,101:105)] #>   1 1 1 1 1 1    1    1         1 1         1         1         1         1 1 #> 1 1 1 1 1 1 0 0.00 0.00 0.0000000 0 0.0000000 0.0000000 0.0000000 0.0000000 0 #> 1 1 1 1 1 1 0 0.00 0.00 0.0000000 0 0.0000000 0.0000000 0.0000000 0.0000000 0 #> 1 1 1 1 1 1 0 0.00 0.00 0.0000000 0 0.0000000 0.0000000 0.0000000 0.0000000 0 #> 1 1 1 1 1 1 0 0.00 0.00 0.0000000 0 0.0000000 0.0000000 0.0000000 0.0000000 0 #> 1 1 1 1 1 1 0 0.00 0.00 0.0000000 0 0.0000000 0.0000000 0.0000000 0.0000000 0 #> 1 0 0 0 0 0 1 1.00 1.00 1.0000000 1 1.0000000 0.0000000 1.0000000 1.0000000 1 #> 1 0 0 0 0 0 1 1.00 1.00 1.0000000 1 0.7500000 1.0000000 0.7500000 0.7500000 1 #> 1 0 0 0 0 0 1 1.00 1.00 1.0000000 1 0.7500000 0.5000000 0.7500000 1.0000000 1 #> 1 0 0 0 0 0 1 1.00 1.00 1.0000000 1 0.6666667 1.0000000 0.6666667 0.6666667 1 #> 1 0 0 0 0 0 1 1.00 1.00 1.0000000 1 0.0000000 1.0000000 0.0000000 0.0000000 1 #> 1 0 0 0 0 0 1 0.75 0.75 0.6666667 0 1.0000000 0.6666667 1.0000000 1.0000000 1 #> 1 0 0 0 0 0 0 1.00 0.50 1.0000000 1 0.6666667 1.0000000 0.6666667 0.5000000 1 #> 1 0 0 0 0 0 1 0.75 0.75 0.6666667 0 1.0000000 0.6666667 1.0000000 1.0000000 1 #> 1 0 0 0 0 0 1 0.75 1.00 0.6666667 0 1.0000000 0.5000000 1.0000000 1.0000000 1 #> 1 0 0 0 0 0 1 1.00 1.00 1.0000000 1 1.0000000 1.0000000 1.0000000 1.0000000 1 consensuscentroid[c(1:5,51:55,101:105),c(1:5,51:55,101:105)] #>   1 1 1 1 1 2         2         2         2         2         2         2 #> 1 1 1 1 1 1 0 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 #> 1 1 1 1 1 1 0 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 #> 1 1 1 1 1 1 0 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 #> 1 1 1 1 1 1 0 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 #> 1 1 1 1 1 1 0 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 #> 2 0 0 0 0 0 1 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 #> 2 0 0 0 0 0 1 1.0000000 0.7500000 1.0000000 1.0000000 0.5000000 0.6666667 #> 2 0 0 0 0 0 1 0.7500000 1.0000000 0.6666667 1.0000000 0.8000000 0.8000000 #> 2 0 0 0 0 0 1 1.0000000 0.6666667 1.0000000 1.0000000 0.6666667 0.8000000 #> 2 0 0 0 0 0 1 1.0000000 1.0000000 1.0000000 1.0000000 0.6666667 0.7500000 #> 2 0 0 0 0 0 1 0.5000000 0.8000000 0.6666667 0.6666667 1.0000000 1.0000000 #> 2 0 0 0 0 0 1 0.6666667 0.8000000 0.8000000 0.7500000 1.0000000 1.0000000 #> 2 0 0 0 0 0 1 1.0000000 0.5000000 1.0000000 1.0000000 0.2500000 0.4000000 #> 2 0 0 0 0 0 1 0.7500000 0.7500000 0.7500000 0.7500000 1.0000000 1.0000000 #> 2 0 0 0 0 0 1 0.7500000 0.6666667 0.7500000 0.6666667 1.0000000 1.0000000 #>           2         2         2 #> 1 0.0000000 0.0000000 0.0000000 #> 1 0.0000000 0.0000000 0.0000000 #> 1 0.0000000 0.0000000 0.0000000 #> 1 0.0000000 0.0000000 0.0000000 #> 1 0.0000000 0.0000000 0.0000000 #> 2 1.0000000 1.0000000 1.0000000 #> 2 1.0000000 0.7500000 0.7500000 #> 2 0.5000000 0.7500000 0.6666667 #> 2 1.0000000 0.7500000 0.7500000 #> 2 1.0000000 0.7500000 0.6666667 #> 2 0.2500000 1.0000000 1.0000000 #> 2 0.4000000 1.0000000 1.0000000 #> 2 1.0000000 0.3333333 0.5000000 #> 2 0.3333333 1.0000000 1.0000000 #> 2 0.5000000 1.0000000 1.0000000 consensuscomplete[c(1:5,51:55,101:105),c(1:5,51:55,101:105)] #>   1 1 1 1 1 1         1         1         1         1         1 1   1         1 #> 1 1 1 1 1 1 0 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0 0.0 0.0000000 #> 1 1 1 1 1 1 0 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0 0.0 0.0000000 #> 1 1 1 1 1 1 0 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0 0.0 0.0000000 #> 1 1 1 1 1 1 0 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0 0.0 0.0000000 #> 1 1 1 1 1 1 0 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0 0.0 0.0000000 #> 1 0 0 0 0 0 1 1.0000000 1.0000000 0.0000000 1.0000000 0.0000000 1 1.0 1.0000000 #> 1 0 0 0 0 0 1 1.0000000 0.7500000 0.0000000 1.0000000 0.3333333 1 0.8 0.6666667 #> 1 0 0 0 0 0 1 0.7500000 1.0000000 0.0000000 0.6666667 0.0000000 1 1.0 1.0000000 #> 1 0 0 0 0 0 0 0.0000000 0.0000000 1.0000000 0.2000000 1.0000000 0 0.0 0.0000000 #> 1 0 0 0 0 0 1 1.0000000 0.6666667 0.2000000 1.0000000 0.0000000 1 0.8 1.0000000 #> 1 0 0 0 0 0 0 0.3333333 0.0000000 1.0000000 0.0000000 1.0000000 0 0.0 0.0000000 #> 1 0 0 0 0 0 1 1.0000000 1.0000000 0.0000000 1.0000000 0.0000000 1 1.0 1.0000000 #> 1 0 0 0 0 0 1 0.8000000 1.0000000 0.0000000 0.8000000 0.0000000 1 1.0 1.0000000 #> 1 0 0 0 0 0 1 0.6666667 1.0000000 0.0000000 1.0000000 0.0000000 1 1.0 1.0000000 #> 1 0 0 0 0 0 1 1.0000000 0.5000000 0.3333333 1.0000000 0.3333333 1 0.5 0.5000000 #>           1 #> 1 0.0000000 #> 1 0.0000000 #> 1 0.0000000 #> 1 0.0000000 #> 1 0.0000000 #> 1 1.0000000 #> 1 1.0000000 #> 1 0.5000000 #> 1 0.3333333 #> 1 1.0000000 #> 1 0.3333333 #> 1 1.0000000 #> 1 0.5000000 #> 1 0.5000000 #> 1 1.0000000"},{"path":"/reference/cooccur.html","id":null,"dir":"Reference","previous_headings":"","what":"Co-occurrence distance for binary/ categorical variables data â cooccur","title":"Co-occurrence distance for binary/ categorical variables data â cooccur","text":"function calculates co-occurrence distance proposed Ahmad Dey (2007).","code":""},{"path":"/reference/cooccur.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Co-occurrence distance for binary/ categorical variables data â cooccur","text":"","code":"cooccur(data)"},{"path":"/reference/cooccur.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Co-occurrence distance for binary/ categorical variables data â cooccur","text":"data matrix data frame binary/ categorical variables (see Details).","code":""},{"path":"/reference/cooccur.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Co-occurrence distance for binary/ categorical variables data â cooccur","text":"Function returns distance matrix (n x n).","code":""},{"path":"/reference/cooccur.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Co-occurrence distance for binary/ categorical variables data â cooccur","text":"function computes co-occurrence distance, binary/ categorical distance, based variable's distribution (see Examples).  Examples, data set: co-occurrence distance transforms category binary/ categorical variable based distribution variables, example, distance categories 1 2 x variable can different distance categories 1 2 z variable. example, transformed distance categories 1 2 z variable presented. cross tabulation z x variables corresponding (column) proportion cross tabulation z y variables corresponding (column) proportion , maximum values proportion row taken 1.0, 0.8, 0.6, 0.5. new distance categories 1 2 z variable $$\\delta_{1,2}^z = \\frac{(1.0+0.8+0.6+0.5) - 2}{2} = 0.45$$ constant \\(2\\) formula applies z variable depends 2 variable distributions, .e x y variables. new distances category x y variables can calculated similar way. Thus, distance objects 1 2 0.45. z variable counted calculate distance objects 1 2 objects 1 2 similar values x y variables. data argument can supplied either matrix data frame, class element integer. integer, converted integer class. data variable , simple matching calculated. co-occurrence absent due dependency distribution variables warning message appears.","code":""},{"path":"/reference/cooccur.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Co-occurrence distance for binary/ categorical variables data â cooccur","text":"Ahmad, ., Dey, L. 2007. K-mean clustering algorithm mixed numeric categorical data. Data Knowledge Engineering 63, pp. 503-527. Harikumar, S., PV, S., 2015. K-medoid clustering heterogeneous data sets. JProcedia Computer Science 70, 226-237.","code":""},{"path":"/reference/cooccur.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Co-occurrence distance for binary/ categorical variables data â cooccur","text":"Weksi Budiaji  Contact: budiaji@untirta.ac.id","code":""},{"path":"/reference/cooccur.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Co-occurrence distance for binary/ categorical variables data â cooccur","text":"","code":"set.seed(1) a <- matrix(sample(1:2, 7*3, replace = TRUE), 7, 3) cooccur(a) #>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] #> [1,] 0.00 1.40 0.95 0.50 0.95 0.00 0.00 #> [2,] 1.40 0.00 0.45 0.90 0.45 1.40 1.40 #> [3,] 0.95 0.45 0.00 0.45 0.90 0.95 0.95 #> [4,] 0.50 0.90 0.45 0.00 0.45 0.50 0.50 #> [5,] 0.95 0.45 0.90 0.45 0.00 0.95 0.95 #> [6,] 0.00 1.40 0.95 0.50 0.95 0.00 0.00 #> [7,] 0.00 1.40 0.95 0.50 0.95 0.00 0.00"},{"path":"/reference/csv.html","id":null,"dir":"Reference","previous_headings":"","what":"Centroid shadow value (CSV) index and plot â csv","title":"Centroid shadow value (CSV) index and plot â csv","text":"function computes centroid shadow values shadow value plots cluster. plot presents mean shadow values well.","code":""},{"path":"/reference/csv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Centroid shadow value (CSV) index and plot â csv","text":"","code":"csv(distdata, idmedoid, idcluster, title = \"\")"},{"path":"/reference/csv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Centroid shadow value (CSV) index and plot â csv","text":"distdata distance matrix (n x n) dist object. idmedoid vector id medoids (see Details). idcluster vector cluster membership (see Details). title title plot.","code":""},{"path":"/reference/csv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Centroid shadow value (CSV) index and plot â csv","text":"Function returns list following components: result data frame shadow values objects plot shadow value plots cluster.","code":""},{"path":"/reference/csv.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Centroid shadow value (CSV) index and plot â csv","text":"origin centroid shadow value calculated shadow function flexclust package, based first second closest centroid. csv function package modifies centroid medoid formula compute shadow value object $$csv() = \\frac{2d(, m())}{d(, m()) + d(, m'())}$$ \\(d(, m())\\) distance object first closest medoid d(, m'()) distance object second closest medoid. idmedoid argument corresponds idcluster argument. length idmedoid 3, example, idcluster 3 unique cluster memberships, returns Error otherwise. length idcluster also equal n (number objects). contrast silhoutte value, centoird shadow value interpreted lower value better cluster separation.","code":""},{"path":"/reference/csv.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Centroid shadow value (CSV) index and plot â csv","text":"F. Leisch. 2010 Neighborhood graphs, stripes shadow plots cluster visualization. Statistics Computing. vol. 20, pp. 457-469 W. Budiaji. 2019 Medoid-based shadow value validation visualization. International Journal Advances Intelligent Informatics.  Vol 5 2 pp. 76-88","code":""},{"path":"/reference/csv.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Centroid shadow value (CSV) index and plot â csv","text":"Weksi Budiaji  Contact: budiaji@untirta.ac.id","code":""},{"path":"/reference/csv.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Centroid shadow value (CSV) index and plot â csv","text":"","code":"distiris <- as.matrix(dist(iris[,1:4])) res <- fastkmed(distiris, 3) sha <- csv(distiris, res$medoid, res$cluster) sha$result[c(1:3,70:75,101:103),] #>           csv cluster #> 1   0.1072151       3 #> 2   0.2456571       3 #> 3   0.2285776       3 #> 70  0.3671597       2 #> 71  0.8882314       1 #> 72  0.4405586       2 #> 73  0.8549667       1 #> 74  0.8216635       2 #> 75  0.7801919       2 #> 101 0.5960123       1 #> 102 0.7947309       1 #> 103 0.5532801       1 sha$plot"},{"path":"/reference/distNumeric.html","id":null,"dir":"Reference","previous_headings":"","what":"A pair distance for numerical variables â distNumeric","title":"A pair distance for numerical variables â distNumeric","text":"function computes pairwise numerical distance two numerical data sets.","code":""},{"path":"/reference/distNumeric.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A pair distance for numerical variables â distNumeric","text":"","code":"distNumeric(x, y, method = \"mrw\", xyequal = TRUE)"},{"path":"/reference/distNumeric.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A pair distance for numerical variables â distNumeric","text":"x first data matrix (see Details). y second data matrix (see Details). method method calculate pairwise numerical distance (see Details). xyequal logical x equal y (see Details).","code":""},{"path":"/reference/distNumeric.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A pair distance for numerical variables â distNumeric","text":"Function returns distance matrix number rows equal number objects x matrix (\\(n_x\\)) number columns equals number objects y matrix (\\(n_y\\)).","code":""},{"path":"/reference/distNumeric.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"A pair distance for numerical variables â distNumeric","text":"x y arguments matrices number columns row indicates object column variable. function calculate pairwise distance rows x y matrices. Although calculates pairwise distance two data sets, default function computes distances x matrix. x matrix equal y matrix, xyequal set FALSE. method available mrw (Manhattan weighted range), sev (squared Euclidean weighted variance), ser (squared Euclidean weighted range), ser.2 (squared Euclidean weighted squared range) se (squared Euclidean). formulas : $$mrw_{ij} = \\sum_{r=1}^{p_n} \\frac{|x_{ir} - x_{jr}|}{R_r}$$ $$sev_{ij} = \\sum_{r=1}^{p_n} \\frac{(x_{ir} - x_{jr})^2}{s_r^2}$$ $$ser_{ij} = \\sum_{r=1}^{p_n} \\frac{(x_{ir} - x_{jr})^2}{ R_r }$$ $$ser.2_{ij} = \\sum_{r=1}^{p_n} \\frac{(x_{ir} - x_{jr})^2}{ R_r^2 }$$ $$se_{ij} = \\sum_{r=1}^{p_n} (x_{ir} - x_{jr})^2$$ \\(p_n\\) number numerical variables, \\(R_r\\) range r-th variables, \\(s_r^2\\) variance r-th variables.","code":""},{"path":"/reference/distNumeric.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"A pair distance for numerical variables â distNumeric","text":"Weksi Budiaji  Contact: budiaji@untirta.ac.id","code":""},{"path":"/reference/distNumeric.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A pair distance for numerical variables â distNumeric","text":"","code":"num <- as.matrix(iris[,1:4]) mrwdist <- distNumeric(num, num, method = \"mrw\") mrwdist[1:6,1:6] #>            [,1]      [,2]      [,3]      [,4]       [,5]      [,6] #> [1,] 0.00000000 0.2638889 0.2530603 0.3225047 0.06944444 0.3841808 #> [2,] 0.26388889 0.0000000 0.1558380 0.1419492 0.27777778 0.6480697 #> [3,] 0.25306026 0.1558380 0.0000000 0.1033427 0.26694915 0.6372411 #> [4,] 0.32250471 0.1419492 0.1033427 0.0000000 0.33639360 0.6727872 #> [5,] 0.06944444 0.2777778 0.2669492 0.3363936 0.00000000 0.3702919 #> [6,] 0.38418079 0.6480697 0.6372411 0.6727872 0.37029190 0.0000000"},{"path":"/reference/distmix.html","id":null,"dir":"Reference","previous_headings":"","what":"Distances for mixed variables data set â distmix","title":"Distances for mixed variables data set â distmix","text":"function computes distance matrix mixed variable data set applying various methods.","code":""},{"path":"/reference/distmix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Distances for mixed variables data set â distmix","text":"","code":"distmix(data, method = \"gower\", idnum = NULL, idbin = NULL, idcat = NULL)"},{"path":"/reference/distmix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Distances for mixed variables data set â distmix","text":"data data frame matrix object. method  method calculate mixed variables distance (see Details). idnum vector column index numerical variables. idbin vector column index binary variables. idcat vector column index categorical variables.","code":""},{"path":"/reference/distmix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Distances for mixed variables data set â distmix","text":"Function returns distance matrix (n x n).","code":""},{"path":"/reference/distmix.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Distances for mixed variables data set â distmix","text":"six methods available calculate mixed variable distance. gower, wishart, podani, huang, harikumar, ahmad. gower Gower (1971) distance common distance mixed variable data set. Although Gower distance accommodates missing values, missing value allowed function. missing value, Gower distance daisy function cluster package can applied. Gower distance objects j computed \\(d_{ij} = 1 - s_{ij}\\), $$s_{ij} = \\frac{\\sum_{l=1}^p \\omega_{ijl} s_{ijl}} {\\sum_{l=1}^p \\omega_{ijl}}$$ \\(\\omega_{ijl}\\) weight variable l usually 1 0 (missing value). variable l numerical variable, $$s_{ijl} = 1- \\frac{|x_{il} - x_{jl}|}{R_l}$$ \\(s_{ijl} \\\\) {0, 1}, variable l binary/ categorical variable. wishart Wishart (2003) proposed different measure compared Gower (1971) numerical variable part. Instead range, applies variance numerical variable \\(s_{ijl}\\) distance becomes $$d_{ij} = \\sqrt{\\sum_{l=1}^p \\omega_{ijl} \\left(\\frac{x_{il} - x_{jl}} {\\delta_{ijl}}\\right)^2}$$ \\(\\delta_{ijl} = s_l\\) l numerical variable \\(\\delta_{ijl} \\\\) {0, 1} l binary/ categorical variable. podani Podani (1999) suggested different method compute distance mixed variable data set. Podani distance calculated $$d_{ij} = \\sqrt{\\sum_{l=1}^p \\omega_{ijl} \\left(\\frac{x_{il} - x_{jl}} {\\delta_{ijl}}\\right)^2}$$ \\(\\delta_{ijl} = R_l\\) l numerical variable \\(\\delta_{ijl} \\\\) {0, 1} l binary/ categorical variable. huang Huang (1997) distance objects j computed $$ d_{ij} = \\sum_{r=1}^{P_n} (x_{ir} - x_{jr})^2 + \\gamma \\sum_{s=1}^{P_c} \\delta_c (x_{} - x_{js})$$ \\(P_n\\) \\(P_c\\) number numerical categorical variables, respectively, $$\\gamma = \\frac{\\sum_{r=1}^{P_n} s_{r}^2}{P_n} $$ \\(\\delta_c(x_{} - x_{js})\\) mismatch/ simple matching distance (see matching) object object j variable s. harikumar Harikumar-PV (2015) proposed distance mixed variable data set: $$ d_{ij} = \\sum_{r=1}^{P_n} |x_{ir} - x_{jr}| + \\sum_{s=1}^{P_c} \\delta_c (x_{} - x_{js}) + \\sum_{t=1}^{p_b} \\delta_b (x_{}, x_{jt})$$ \\(P_b\\) number binary variables, \\(\\delta_c (x_{}, x_{js})\\) co-occurrence distance (see cooccur), \\(\\delta_b (x_{}, x_{jt})\\) Hamming distance. ahmad Ahmad Dey (2007) computed distance mixed variable set via $$ d_{ij} = \\sum_{r=1}^{P_n} (x_{ir} - x_{jr})^2 + \\sum_{s=1}^{P_c} \\delta_c (x_{} - x_{js})$$ \\(\\delta_c (x_{}, x_{jt})\\) co-occurrence distance (see cooccur). Ahmad Dey distance, binary categorical variables separable co-occurrence distance based combined two classes, .e. binary categorical variables. Note function applies standard version Squared Euclidean, .e without weight. leas two arguments idnum, idbin, idcat provided function calculates mixed distance. method harikumar, categorical variables least two variables co-occurrence distance can computed. also applies method = \"ahmad\". idbin + idcat 1 column. returns Error message otherwise.","code":""},{"path":"/reference/distmix.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Distances for mixed variables data set â distmix","text":"Ahmad, ., Dey, L. 2007. K-mean clustering algorithm mixed numeric categorical data. Data Knowledge Engineering 63, pp. 503-527. Gower, J., 1971. general coefficient similarity properties. Biometrics 27, pp. 857-871 Harikumar, S., PV, S., 2015. K-medoid clustering heterogeneous data sets. JProcedia Computer Science 70, pp. 226-237. Huang, Z., 1997. Clustering large data sets mixed numeric categorical values, : First Pacific-Asia Conference Knowledge Discovery Data Mining, pp. 21-34. Podani, J., 1999. Extending gower's general coefficient similarity ordinal characters. Taxon 48, pp. 331-340. Wishart, D., 2003. K-means clustering outlier detection, mixed variables missing values, : Exploratory Data Analysis Empirical Research: Proceedings 25th Annual Conference Gesellschaft fur Klassifikation e.V., University Munich, March 14-16, 2001, Springer Berlin Heidelberg, Berlin, Heidelberg. pp. 216-226.","code":""},{"path":"/reference/distmix.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Distances for mixed variables data set â distmix","text":"Weksi Budiaji  Contact: budiaji@untirta.ac.id","code":""},{"path":"/reference/distmix.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Distances for mixed variables data set â distmix","text":"","code":"set.seed(1) a <- matrix(sample(1:2, 7*3, replace = TRUE), 7, 3) a1 <- matrix(sample(1:3, 7*3, replace = TRUE), 7, 3) mixdata <- cbind(iris[1:7,1:3], a, a1) colnames(mixdata) <- c(paste(c(\"num\"), 1:3, sep = \"\"),                        paste(c(\"bin\"), 1:3, sep = \"\"),                        paste(c(\"cat\"), 1:3, sep = \"\")) distmix(mixdata, method = \"gower\", idnum = 1:3, idbin = 4:6, idcat = 7:9) #>           1         2         3         4         5         6         7 #> 1 0.0000000 0.7561728 0.6759259 0.5910494 0.4706790 0.3966049 0.3040123 #> 2 0.7561728 0.0000000 0.1913580 0.3040123 0.3101852 0.9305556 0.5354938 #> 3 0.6759259 0.1913580 0.0000000 0.1929012 0.4521605 0.8503086 0.3996914 #> 4 0.5910494 0.3040123 0.1929012 0.0000000 0.3672840 0.7098765 0.2870370 #> 5 0.4706790 0.3101852 0.4521605 0.3672840 0.0000000 0.7314815 0.3024691 #> 6 0.3966049 0.9305556 0.8503086 0.7098765 0.7314815 0.0000000 0.5895062 #> 7 0.3040123 0.5354938 0.3996914 0.2870370 0.3024691 0.5895062 0.0000000"},{"path":"/reference/fastkmed.html","id":null,"dir":"Reference","previous_headings":"","what":"Simple and fast k-medoid algorithm â fastkmed","title":"Simple and fast k-medoid algorithm â fastkmed","text":"function runs simple fast k-medoid algorithm proposed Park Jun (2009).","code":""},{"path":"/reference/fastkmed.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simple and fast k-medoid algorithm â fastkmed","text":"","code":"fastkmed(distdata, ncluster, iterate = 10, init = NULL)"},{"path":"/reference/fastkmed.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simple and fast k-medoid algorithm â fastkmed","text":"distdata distance matrix (n x n) dist object. ncluster number clusters. iterate number iterations clustering algorithm. init vector initial objects cluster medoids (see Details).","code":""},{"path":"/reference/fastkmed.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simple and fast k-medoid algorithm â fastkmed","text":"Function returns list components: cluster clustering memberships result. medoid id medoids. minimum_distance distance objects cluster medoid.","code":""},{"path":"/reference/fastkmed.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Simple and fast k-medoid algorithm â fastkmed","text":"simple fast k-medoids, sets set medoids cluster centers, adapts k-means algorithm medoid -dating. new medoids iteration calculated within cluster gains speed. init = NULL required Park Jun (2009) particular method select initial medoids.  initial medoids selected $$ v_j = \\sum_{=1}^n \\frac{d_{ij}}{\\sum_{l=1}^n d_{il}}, \\quad j = 1, 2, 3, \\ldots, n $$ first k \\(v_j\\) selected number clusters k. init can provided vector id objects. length vector equal number clusters. However, assigning vector init argument, algorithm longer simple fast k-medoids algorithm. inckmed function, example, defines different method select initial medoid though applies fastkmed function.","code":""},{"path":"/reference/fastkmed.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Simple and fast k-medoid algorithm â fastkmed","text":"Park, H., Jun, C., 2009. simple fast algorithm k-medoids clustering. Expert Systems Applications 36, pp. 3336-3341.","code":""},{"path":"/reference/fastkmed.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Simple and fast k-medoid algorithm â fastkmed","text":"Weksi Budiaji  Contact: budiaji@untirta.ac.id","code":""},{"path":"/reference/fastkmed.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simple and fast k-medoid algorithm â fastkmed","text":"","code":"num <- as.matrix(iris[,1:4]) mrwdist <- distNumeric(num, num, method = \"mrw\") result <- fastkmed(mrwdist, ncluster = 3, iterate = 50) table(result$cluster, iris[,5]) #>     #>     setosa versicolor virginica #>   1     50          0         0 #>   2      0         39         3 #>   3      0         11        47"},{"path":"/reference/globalfood.html","id":null,"dir":"Reference","previous_headings":"","what":"Global food security index â globalfood","title":"Global food security index â globalfood","text":"dataset containing four variables 113 countries food security index based panelists evaluation 2017.","code":""},{"path":"/reference/globalfood.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Global food security index â globalfood","text":"","code":"globalfood"},{"path":"/reference/globalfood.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Global food security index â globalfood","text":"data frame 113 rows 4 variables: affordability Index food affordability. availability Index food availability. safety Index food quality safety. resilience Index natural resources resilience.","code":""},{"path":"/reference/globalfood.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Global food security index â globalfood","text":"original indicator variables consist 27 variables. , summarized four pillars food security; affordability, availability, quality safety, natural resources resilience. Food-security expertise panelists evaluate score country 0 100, 0 least favorable towards food security. https://impact.economist.com/sustainability/project/food-security-index/","code":""},{"path":"/reference/heart.html","id":null,"dir":"Reference","previous_headings":"","what":"Heart Disease data set â heart","title":"Heart Disease data set â heart","text":"mixed variable dataset containing 14 variables 297 patients heart disease diagnosis.","code":""},{"path":"/reference/heart.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Heart Disease data set â heart","text":"","code":"heart"},{"path":"/reference/heart.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Heart Disease data set â heart","text":"data frame 297 rows 14 variables: age Age years (numerical). sex Sex: 1 = male, 0 = female (logical). cp Four chest pain types: (1) typical angina, (2) atypical angina   (3)non-anginal pain, (4) asymptomatic (categorical). trestbps Resting blood pressure (mm Hg admission   hospital) (numerical). chol Serum cholestoral mg/dl (numerical). fbs Fasting blood sugar 120 mg/dl (logical). restecg Resting electrocardiographic results: (0) normal,   (1) ST-T wave abnormality, (2) showing probable definite   left ventricular hypertrophy Estes' criteria (categorical). thalach Maximum heart rate achieved (numerical). exang Exercise induced angina (logical). oldpeak ST depression induced exercise relative   rest (numerical). slope slope peak exercise ST segment: (1) upsloping,   (2) flat, (3) downsloping (categorical). ca Number major vessels (0-3) colored flourosopy (numerical). thal (3) normal, (6) fixed defect, (7) reversable defect   (categorical). class Diagonosis heart disease (4 classes). can 2 classes   setting 0 0 values 1 non-0 values.","code":""},{"path":"/reference/heart.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Heart Disease data set â heart","text":"data set taken machine learning repository UCI. original data set consists 303 patients 6 NA's. , missing values omitted reduces 297 patients. https://archive.ics.uci.edu/ml/datasets/Heart+Disease","code":""},{"path":"/reference/heart.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Heart Disease data set â heart","text":"Lichman, M. (2013). UCI machine learning repository.","code":""},{"path":"/reference/inckmed.html","id":null,"dir":"Reference","previous_headings":"","what":"Increasing number of clusters in k-medoids algorithm â inckmed","title":"Increasing number of clusters in k-medoids algorithm â inckmed","text":"function runs increasing number  clusters k-medoids algorithm proposed Yu et. al. (2018).","code":""},{"path":"/reference/inckmed.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Increasing number of clusters in k-medoids algorithm â inckmed","text":"","code":"inckmed(distdata, ncluster, iterate = 10, alpha = 1)"},{"path":"/reference/inckmed.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Increasing number of clusters in k-medoids algorithm â inckmed","text":"distdata distance matrix (n x n) dist object. ncluster number clusters. iterate number iterations clustering algorithm. alpha stretch factor determine range initial medoid selection (see Details).","code":""},{"path":"/reference/inckmed.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Increasing number of clusters in k-medoids algorithm â inckmed","text":"Function returns list components: cluster clustering memberships result. medoid id medoids. minimum_distance distance objects cluster medoid.","code":""},{"path":"/reference/inckmed.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Increasing number of clusters in k-medoids algorithm â inckmed","text":"algorithm claimed manage weakness simple fast-kmedoids (fastkmed). origin algorithm centroid-based algorithm applying Euclidean distance. , Bbecause function medoid-based algorithm, object mean (centroid) variance redefined medoid deviation, respectively. alpha argument stretch factor, .e. constant defined user. applied determine set medoid candidates. medoid candidates calculated \\(O_c = \\){\\(X_i\\)| \\(\\sigma_i \\leq \\alpha \\sigma, = 1, 2, \\ldots, n\\) }, \\(\\sigma_i\\)  average deviation object , \\(\\sigma\\) average deviation data set. computed $$\\sigma = \\sqrt{\\frac{1}{n-1} \\sum_{=1}^n d(O_i, v_1)}$$ $$\\sigma_i = \\sqrt{\\frac{1}{n-1} \\sum_{=1}^n d(O_i, O_j)}$$ n number objects, \\(O_i\\) object , \\(v_1\\) centrally located object.","code":""},{"path":"/reference/inckmed.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Increasing number of clusters in k-medoids algorithm â inckmed","text":"Yu, D., Liu, G., Guo, M., Liu, X., 2018. improved K-medoids algorithm based step increasing optimizing medoids. Expert Systems Applications 92, pp. 464-473.","code":""},{"path":"/reference/inckmed.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Increasing number of clusters in k-medoids algorithm â inckmed","text":"Weksi Budiaji  Contact: budiaji@untirta.ac.id","code":""},{"path":"/reference/inckmed.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Increasing number of clusters in k-medoids algorithm â inckmed","text":"","code":"num <- as.matrix(iris[,1:4]) mrwdist <- distNumeric(num, num, method = \"mrw\") result <- inckmed(mrwdist, ncluster = 3, iterate = 50, alpha = 1.5) table(result$cluster, iris[,5]) #>     #>     setosa versicolor virginica #>   1     50          0         0 #>   2      0         46        11 #>   3      0          4        39"},{"path":"/reference/matching.html","id":null,"dir":"Reference","previous_headings":"","what":"A pair distance for binary/ categorical variables â matching","title":"A pair distance for binary/ categorical variables â matching","text":"function computes simple matching distance two data frames/ matrices.","code":""},{"path":"/reference/matching.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A pair distance for binary/ categorical variables â matching","text":"","code":"matching(x, y)"},{"path":"/reference/matching.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A pair distance for binary/ categorical variables â matching","text":"x first data frame matrix (see Details). y second data frame matrix (see Details).","code":""},{"path":"/reference/matching.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A pair distance for binary/ categorical variables â matching","text":"Function returns distance matrix number rows equal number objects x data frame/ matrix (\\(n_x\\)) number columns equals number objects y data frame/ matrix (\\(n_y\\)).","code":""},{"path":"/reference/matching.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"A pair distance for binary/ categorical variables â matching","text":"x y arguments data frames/ matrices number columns row indicates object column variable. function calculates pairwise distance rows x y data frames/ matrices. x data frame/ matrix equal y data frame/ matrix, explicitly calculates distances x data frame/ matrix. simple matching distance objects j calculated $$d_{ij} = \\frac{\\sum_{s=1}^{P}(x_{}-x_{js})}{P}$$  \\(P\\) number variables, \\( x_{}-x_{js} \\\\) {0, 1}. \\( x_{}-x_{js} = 0\\), \\( x_{}=x_{js}\\) \\(x_{}-x_{js} = 1\\), \\(x_{} \\neq x_{js}\\). example, distance objects 1 2 presented. distance objects 1 2 $$d_{12} = \\frac{\\sum_{s=1}^{3}(x_{}-x_{js})}{3} = \\frac{0 + 0 + 1}{3} = \\frac{1}{3} = 0.33$$","code":""},{"path":"/reference/matching.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"A pair distance for binary/ categorical variables â matching","text":"Weksi Budiaji  Contact: budiaji@untirta.ac.id","code":""},{"path":"/reference/matching.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A pair distance for binary/ categorical variables â matching","text":"","code":"set.seed(1) a <- matrix(sample(1:2, 7*3, replace = TRUE), 7, 3) matching(a, a) #>           [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7] #> [1,] 0.0000000 1.0000000 0.6666667 0.3333333 0.6666667 0.0000000 0.0000000 #> [2,] 1.0000000 0.0000000 0.3333333 0.6666667 0.3333333 1.0000000 1.0000000 #> [3,] 0.6666667 0.3333333 0.0000000 0.3333333 0.6666667 0.6666667 0.6666667 #> [4,] 0.3333333 0.6666667 0.3333333 0.0000000 0.3333333 0.3333333 0.3333333 #> [5,] 0.6666667 0.3333333 0.6666667 0.3333333 0.0000000 0.6666667 0.6666667 #> [6,] 0.0000000 1.0000000 0.6666667 0.3333333 0.6666667 0.0000000 0.0000000 #> [7,] 0.0000000 1.0000000 0.6666667 0.3333333 0.6666667 0.0000000 0.0000000"},{"path":"/reference/msv.html","id":null,"dir":"Reference","previous_headings":"","what":"Medoid shadow value (MSV) index and plot â msv","title":"Medoid shadow value (MSV) index and plot â msv","text":"function computes medoid shadow values shadow value plots cluster. plot presents mean shadow values well.","code":""},{"path":"/reference/msv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Medoid shadow value (MSV) index and plot â msv","text":"","code":"msv(distdata, idmedoid, idcluster, title = \"\")"},{"path":"/reference/msv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Medoid shadow value (MSV) index and plot â msv","text":"distdata distance matrix (n x n) dist object. idmedoid vector id medoids (see Details). idcluster vector cluster membership (see Details). title title plot.","code":""},{"path":"/reference/msv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Medoid shadow value (MSV) index and plot â msv","text":"Function returns list following components: result data frame shadow values objects plot shadow value plots cluster.","code":""},{"path":"/reference/msv.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Medoid shadow value (MSV) index and plot â msv","text":"origin shadow value calculated shadow function flexclust package, based first second closest centroid. msv function package modifies centroid medoid formula compute shadow value object $$msv() = \\frac{d(, m'())-d(, m())}{d(, m'())}$$ \\(d(, m())\\) distance object first closest medoid d(, m'()) distance object second closest medoid. idmedoid argument corresponds idcluster argument. length idmedoid 3, example, idcluster 3 unique cluster memberships, returns Error otherwise. length idcluster also equal n (number objects). contrast centroid shadow value, medoid shadow value interpreted likewise silhoutte value, higher value better separation.","code":""},{"path":"/reference/msv.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Medoid shadow value (MSV) index and plot â msv","text":"F. Leisch. 2010 Neighborhood graphs, stripes shadow plots cluster visualization. Statistics Computing. vol. 20, pp. 457-469 W. Budiaji. 2019 Medoid-based shadow value validation visualization. International Journal Advances Intelligent Informatics.  Vol 5 2 pp. 76-88","code":""},{"path":"/reference/msv.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Medoid shadow value (MSV) index and plot â msv","text":"Weksi Budiaji  Contact: budiaji@untirta.ac.id","code":""},{"path":"/reference/msv.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Medoid shadow value (MSV) index and plot â msv","text":"","code":"distiris <- as.matrix(dist(iris[,1:4])) res <- fastkmed(distiris, 3) sha <- msv(distiris, res$medoid, res$cluster) sha$result[c(1:3,70:75,101:103),] #>           msv cluster #> 1   0.9433559       3 #> 2   0.8599720       3 #> 3   0.8709638       3 #> 70  0.7751405       2 #> 71  0.2010645       1 #> 72  0.7174895       2 #> 73  0.2533260       1 #> 74  0.3026919       2 #> 75  0.3603979       2 #> 101 0.5754861       1 #> 102 0.3406195       1 #> 103 0.6175624       1 sha$plot"},{"path":"/reference/pcabiplot.html","id":null,"dir":"Reference","previous_headings":"","what":"Biplot of a PCA object â pcabiplot","title":"Biplot of a PCA object â pcabiplot","text":"function creates biplot pca object, generated prcomp function stats package.","code":""},{"path":"/reference/pcabiplot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Biplot of a PCA object â pcabiplot","text":"","code":"pcabiplot(   PC,   x = \"PC1\",   y = \"PC2\",   var.line = TRUE,   colobj = rep(1, nrow(PC$x)),   o.size = 1 )"},{"path":"/reference/pcabiplot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Biplot of a PCA object â pcabiplot","text":"PC pca object generated prcomp function. x X axis (see Details). y Y axis (see Details). var.line logical input, variable lines plotted. colobj vector provide color objects (see Details). o.size numeric number set object size.","code":""},{"path":"/reference/pcabiplot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Biplot of a PCA object â pcabiplot","text":"Function returns plot pca.","code":""},{"path":"/reference/pcabiplot.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Biplot of a PCA object â pcabiplot","text":"function plot pca biplot pca object. x y axes can supplied principle component. length colobj vector equal number objects. argument controls color objects convenient explore clustering result. default value object color.","code":""},{"path":"/reference/pcabiplot.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Biplot of a PCA object â pcabiplot","text":"Weksi Budiaji  Contact: budiaji@untirta.ac.id","code":""},{"path":"/reference/pcabiplot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Biplot of a PCA object â pcabiplot","text":"","code":"pcadat <- prcomp(iris[,1:4], scale. = TRUE) pcabiplot(pcadat)"},{"path":"/reference/rankkmed.html","id":null,"dir":"Reference","previous_headings":"","what":"Rank k-medoid algorithm â rankkmed","title":"Rank k-medoid algorithm â rankkmed","text":"function runs rank k-medoids algorithm proposed Zadegan et. al. (2013).","code":""},{"path":"/reference/rankkmed.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rank k-medoid algorithm â rankkmed","text":"","code":"rankkmed(distdata, ncluster, m = 3, iterate = 10, init = NULL)"},{"path":"/reference/rankkmed.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rank k-medoid algorithm â rankkmed","text":"distdata distance matrix (n x n) dist object. ncluster number clusters. m number objects compute hostility (see Details). iterate number iterations clustering algorithm. init vector initial objects cluster medoids (see Details).","code":""},{"path":"/reference/rankkmed.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rank k-medoid algorithm â rankkmed","text":"Function returns list components: cluster clustering memberships result. medoid id medoids. minimum_distance distance objects cluster medoid.","code":""},{"path":"/reference/rankkmed.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Rank k-medoid algorithm â rankkmed","text":"algorithm claimed cope local optima problem simple fast-kmedoids algorithm (fastkmed). m argument defined user \\(1 < m \\leq n\\). m hostility measure computed $$m_i = \\sum_{X_j \\Y} r_{ij}$$ \\(x_j\\) object j, Y set objects many m, \\(r_{ij}\\) rank distance, .e. sorted distance, object j. init can provided vector id objects. length vector equal number clusters. However, assigning vector init argument, algorithm longer rank k-medoids algorithm.","code":""},{"path":"/reference/rankkmed.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Rank k-medoid algorithm â rankkmed","text":"Zadegan, S.M.R, Mirzaie M, Sadoughi, F. 2013. Ranked k-medoids: fast accurate rank-based partitioning algorithm clustering large datasets. Knowledge-Based Systems 39, 133-143.","code":""},{"path":"/reference/rankkmed.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Rank k-medoid algorithm â rankkmed","text":"Weksi Budiaji  Contact: budiaji@untirta.ac.id","code":""},{"path":"/reference/rankkmed.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Rank k-medoid algorithm â rankkmed","text":"","code":"num <- as.matrix(iris[,1:4]) mrwdist <- distNumeric(num, num, method = \"mrw\") result <- rankkmed(mrwdist, ncluster = 3, iterate = 50) table(result$cluster, iris[,5]) #>     #>     setosa versicolor virginica #>   1     50          0         0 #>   2      0         48        16 #>   3      0          2        34"},{"path":"/reference/sil.html","id":null,"dir":"Reference","previous_headings":"","what":"Silhouette index and plot â sil","title":"Silhouette index and plot â sil","text":"function creates silhouette indices silhouette plots cluster. plot presents also mean silhouette indices per cluster.","code":""},{"path":"/reference/sil.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Silhouette index and plot â sil","text":"","code":"sil(distdata, idmedoid, idcluster, title = \"\")"},{"path":"/reference/sil.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Silhouette index and plot â sil","text":"distdata distance matrix (n x n) dist object. idmedoid vector id medoids (see Details). idcluster vector cluster membership (see Details). title title plot.","code":""},{"path":"/reference/sil.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Silhouette index and plot â sil","text":"Function returns list following components: result data frame silhouette indices objects plot silhouette plots cluster.","code":""},{"path":"/reference/sil.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Silhouette index and plot â sil","text":"silhouette index object calculated $$si()=\\frac{b_i-a_i}{max(a_i, b_i)}$$ \\(a_i\\) average distance object objects within cluster, \\(b_i\\) average distance object objects within nearest cluster. idmedoid argument corresponds idcluster argument. length idmedoid 3, example, idcluster 3 unique memberships, returns Error otherwise. length idcluster also equal n (number objects).","code":""},{"path":"/reference/sil.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Silhouette index and plot â sil","text":"P. J. Rousseeuw. 1987 Silhouettes: graphical aid interpretation validation cluster analysis. Journal Computational Applied Mathematics, vol. 20, pp. 53-65","code":""},{"path":"/reference/sil.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Silhouette index and plot â sil","text":"Weksi Budiaji  Contact: budiaji@untirta.ac.id","code":""},{"path":"/reference/sil.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Silhouette index and plot â sil","text":"","code":"distiris <- as.matrix(dist(iris[,1:4])) res <- fastkmed(distiris, 3) silhouette <- sil(distiris, res$medoid, res$cluster) silhouette$result[c(1:3,70:75,101:103),] #>      silhouette cluster #> 1    0.83673357       3 #> 2    0.79331544       3 #> 3    0.80997792       3 #> 70   0.66754095       2 #> 71  -0.03063885       1 #> 72   0.57112907       2 #> 73   0.01773359       1 #> 74   0.30800460       2 #> 75   0.35178413       2 #> 101  0.49020081       1 #> 102  0.12628381       1 #> 103  0.55530899       1 silhouette$plot"},{"path":"/reference/skm.html","id":null,"dir":"Reference","previous_headings":"","what":"Simple k-medoid algorithm â skm","title":"Simple k-medoid algorithm â skm","text":"function runs simple k-medoid algorithm proposed Budiaji Leisch (2019).","code":""},{"path":"/reference/skm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simple k-medoid algorithm â skm","text":"","code":"skm(distdata, ncluster, seeding = 20, iterate = 10)"},{"path":"/reference/skm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simple k-medoid algorithm â skm","text":"distdata distance matrix (n x n) dist object. ncluster number clusters. seeding number seedings run algorithm (see Details). iterate number iterations seeding (see Details).","code":""},{"path":"/reference/skm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simple k-medoid algorithm â skm","text":"Function returns list components: cluster clustering memberships result. medoid id medoids. minimum_distance distance objects cluster medoid.","code":""},{"path":"/reference/skm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Simple k-medoid algorithm â skm","text":"simple k-medoids, sets set medoids cluster centers, adapts simple fast k-medoid algoritm. best practice run simple fast k-medoid running algorithm several times different random seeding options.","code":""},{"path":"/reference/skm.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Simple k-medoid algorithm â skm","text":"W. Budiaji, F. Leisch. 2019. Simple K-Medoids Partitioning Algorithm Mixed Variable Data. Algorithms Vol 12(9) 177","code":""},{"path":"/reference/skm.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Simple k-medoid algorithm â skm","text":"Weksi Budiaji  Contact: budiaji@untirta.ac.id","code":""},{"path":"/reference/skm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simple k-medoid algorithm â skm","text":"","code":"num <- as.matrix(iris[,1:4]) mrwdist <- distNumeric(num, num, method = \"mrw\") result <- skm(mrwdist, ncluster = 3, seeding = 50) table(result$cluster, iris[,5]) #>     #>     setosa versicolor virginica #>   1     50          0         0 #>   2      0          4        39 #>   3      0         46        11"},{"path":"/news/index.html","id":"kmed-042","dir":"Changelog","previous_headings":"","what":"kmed 0.4.2","title":"kmed 0.4.2","text":"CRAN release: 2022-08-29 Edited source URL global food data. Fixed pcabiplot.","code":""},{"path":"/news/index.html","id":"kmed-041","dir":"Changelog","previous_headings":"","what":"kmed 0.4.1","title":"kmed 0.4.1","text":"Adjusted Katex notes. Added note distmix, ahmad dey distance. Edited abstract. Edited NEWS.md.","code":""},{"path":"/news/index.html","id":"kmed-040","dir":"Changelog","previous_headings":"","what":"kmed 0.4.0","title":"kmed 0.4.0","text":"CRAN release: 2021-01-04 Added msv. Added skm. Fixed csv. Fixed inckmed. Fixed rankkmed. Fixed fastkmed. Fixed sil. Deleted silhouette. Deleted shadow. Deleted stepkmed. Edited NEWS.md. Edited vignette.","code":""},{"path":"/news/index.html","id":"kmed-030","dir":"Changelog","previous_headings":"","what":"kmed 0.3.0","title":"kmed 0.3.0","text":"CRAN release: 2019-06-14 Added clust5. Fixed clustboot. Fixed cooccur. Fixed distmix. Fixed fastkmed. Fixed rankkmed. Fixed shadow. Fixed silhoutte. Fixed stepkmed. Deprecated . Edited NEWS.md. Edited vignette.","code":""},{"path":"/news/index.html","id":"kmed-020","dir":"Changelog","previous_headings":"","what":"kmed 0.2.0","title":"kmed 0.2.0","text":"CRAN release: 2019-01-02 Added barplotnum make barplot numerical data set. Added pcabiplot make biplot pca class. Added silhoutte obtain silhoutte index plot. Added shadow obtain shadow value index medoid instead centroid plot. Added two data sets. Fixed cooccur. Fixed clustboot. Fixed fastkmed. Fixed matching. Fixed stepkmed. Fixed rankkmed. Deleted coocurance. Edited NEWS.md. Edited vignette.","code":""},{"path":"/news/index.html","id":"kmed-010","dir":"Changelog","previous_headings":"","what":"kmed 0.1.0","title":"kmed 0.1.0","text":"CRAN release: 2018-08-07 Added NEWS.md file track changes package. Added step k-medoid rank k-medoid algorithms. Added ahmad dey distance mixed variables. Edited vignette. Deprecated coocurance.","code":""},{"path":"/news/index.html","id":"kmed-001","dir":"Changelog","previous_headings":"","what":"kmed 0.0.1","title":"kmed 0.0.1","text":"CRAN release: 2018-02-12 Description version number fixed.","code":""},{"path":"/news/index.html","id":"kmed-0009000","dir":"Changelog","previous_headings":"","what":"kmed 0.0.0.9000","title":"kmed 0.0.0.9000","text":"First submitted cran","code":""}]
